from typing import Callable, Any, List, Literal

import os 
import psutil
import functools
import pickle

import pandas as pd
import xarray as xr
import arraylake as al

from config import CACHE_DIR, ARRAYLAKE_REPO

def safe_reindex(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
    assert df1.index.isin(df2.index).all()
    return df1.reindex(df2.index)

def xr_pct_change(da: xr.DataArray, dim: str, periods: int = 1) -> xr.DataArray:
    """
    Calculate the percent change between values of an xarray DataArray along a specified dimension,
    similar to the pandas pct_change function.

    Parameters
    ----------
    da : xr.DataArray
        The input xarray DataArray.
    dim : str
        The dimension along which to calculate the percent change.
    periods : int, optional
        The number of periods to shift for calculating percent change, by default 1.

    Returns
    -------
    xr.DataArray
        DataArray of the same shape with percent changes computed along the specified dimension.

    Examples
    --------
    >>> da = xr.DataArray([1, 2, 4, 8], dims='time')
    >>> xr_pct_change(da, dim='time')
    <xarray.DataArray (time: 4)>
    array([nan, 1. , 1. , 1. ])
    Dimensions without coordinates: time

    Notes
    -----
    - Generated by ChatGPT (v2.0) on 2024-10-22.
    - Prompt: "Please write a pct_change function for an xarray DataArray in the style of pandas pct_change function.
      Please be careful to note that the diff function in xarray uses the numpy convention, not the pandas convention."
    """
    shifted = da.shift({dim: periods})
    pct_change = (da - shifted) / shifted
    # pct_change = pct_change.where(~isinf(pct_change))  # Handle division by zero cases
    return pct_change


def format_date(date_str):
    return pd.to_datetime(date_str).strftime('%m/%d')


def check_memory_usage():
    '''Returns memory usage in MB'''
    memory_info = psutil.Process().memory_info()
    return int(memory_info.rss / 1024 ** 2) + 1
    # print(f"Memory usage: {memory_info.rss / 1024 ** 2:.2f} MB")


def summarize_memory_usage():
    '''Summarizes memory in GB'''
    _dict = ({'Process usage':    psutil.Process().memory_info().rss,
              'System available': psutil.virtual_memory().available,
              'System total':     psutil.virtual_memory().total,
              })
    return (pd.Series(name ='Memory Usage (GB)',
                      data = _dict)
            .div(1024 ** 3)
            .round(3))


def write_pickle(obj: Any, path: str) -> None:
    with open(path, 'wb') as f:
        pickle.dump(obj, f, protocol=-1)

def read_pickle(path: str) -> Any:
    with open(path, 'rb') as f:
        return pickle.load(f)

def write_zarr(ds: xr.Dataset, path: str) -> None:
    ds.to_zarr(path, mode='w')

def read_zarr(path: str) -> xr.Dataset:
    return xr.open_zarr(path)

def read_file(path: str, type: str) -> Any:
    type_dict = {'pkl':  read_pickle, 
                 'zarr': read_zarr}
    return type_dict[type](path)    

def write_file(obj: Any, path: str, type: str) -> None:
    type_dict = {'pkl':  write_pickle, 
                 'zarr': write_zarr}
    return type_dict[type](obj, path)

def read_arraylake(repo_name: str | None = None) -> xr.Dataset:
    if repo_name is None:
        repo_name = ARRAYLAKE_REPO
    client = al.Client()
    repo = client.get_repo(repo_name)
    session = repo.readonly_session("main")
    return xr.open_zarr(session.store, consolidated=False)

def write_arraylake(ds: xr.Dataset, repo_name: str | None = None,
                    commit_message: str = 'Update dataset') -> None:
    if repo_name is None:
        repo_name = ARRAYLAKE_REPO
    client = al.Client()
    repo = client.get_repo(repo_name)
    session = repo.writable_session("main")
    ds.to_zarr(session.store, mode='a', consolidated=False)
    session.commit(commit_message)

def cache_to_arraylake(func: Callable) -> Callable:
    # TODO: complete this function
    # TODO: check staleness by just querying date, not whole dataset
    def wrapper(*args: List[Any], 
                read_cache: bool = True, 
                write_cache: bool = True, 
                repo_name: str | None = None, 
                check: Callable | None = None,
                **kwargs: List[Any]) -> Any:
        if check is None:
            check = lambda x: True
        if read_cache:
            data = read_arraylake(repo_name)
            if not check(data):
                data = func(*args, **kwargs)
            if write_cache:
                write_arraylake(data, repo_name)
        else:
            data = func(*args, **kwargs)
            if write_cache:
                write_arraylake(data, repo_name)
        return data
    return wrapper


def cache_to_file(func: Callable) -> Callable:
    """
    A decorator to cache the result of a function to a file. If the cache file exists, 
    the result is read from the file instead of calling the function. If the cache file 
    does not exist, the function is called and the result is written to the cache file.

    Parameters
    ----------
    func : Callable
        The function to be decorated.
    *args : List[Any]
        Positional arguments to pass to the function.
    read_cache : bool, optional
        Whether to read from the cache if it exists. Defaults to True.
    write_cache : bool, optional
        Whether to write the result to the cache. Defaults to True.
    cache_dir : str, optional
        Directory where the cache file is stored. Defaults to 'cache'.
    cache_file : str, optional
        Name of the cache file. Defaults to None, which means the cache file name will be derived from the function name.
    check : callable, optional
        A function to validate the cached data. Defaults to a function that always returns True. If False, the function will be recomputed.
    file_type : Literal['pkl', 'zarr'], optional
        The file type for the cache file. Defaults to 'pkl'.
    **kwargs : List[Any]
        Keyword arguments to pass to the function.

    Returns
    -------
    Callable
        The result of the function, either from the cache or freshly computed.

    Notes
    -----
    The cache file is stored in the specified cache directory with a default name 
    based on the function name and a specified file type extension (default is '.pkl'). 
    The cache directory, file name, and file type can be customized through the 
    decorator's parameters.

    Examples
    --------
    >>> @cache_to_file
    ... def expensive_function(x):
    ...     # Some expensive computation
    ...     return x * x
    >>> result = expensive_function(2)
    """
    # TODO: Complete arraylake cache... or actually make a different decorator
    def wrapper(*args: List[Any], 
                read_cache=True, write_cache=True, 
                cache_dir=None, cache_file=None, 
                check=None,
                file_type: Literal['pkl', 'zarr'] = 'pkl',
                **kwargs: List[Any]) -> Any:
        
        if cache_dir is None:
            cache_dir = CACHE_DIR
        if check is None:
            check = lambda x: True
        if cache_file is None:
            cache_file = f'{func.__name__}.{file_type}'
        cache_path = os.path.join(cache_dir, cache_file)
        
        if read_cache and os.path.exists(cache_path):
            data = read_file(cache_path, file_type)
            if not check(data):
                data = func(*args, **kwargs) # TODO: pass read_cache=False
                if write_cache:
                    write_file(data, cache_path, file_type)
        else:
            data = func(*args, **kwargs)
            if write_cache:
                write_file(data, cache_path, file_type)
        return data
    return wrapper
