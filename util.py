from typing import Callable, Any, List, Literal, Optional, Iterable, Tuple, Dict

import os 
import psutil
import functools
import pickle

from datetime import datetime

from pandas.tseries.offsets import BDay

import pandas as pd
import xarray as xr
import streamlit as st

from config import CACHE_DIR, ARRAYLAKE_REPO
from config import TRAILING_WINDOWS, MARKET_EVENTS


# Patch `pydantic`, used by `streamlit` server, which doesn't recognize `DBIDBytes` type from `arraylake`.
# Patch generated by ChatGPT
from pydantic import BaseModel
class MyModel(BaseModel):
    db_id: "DBIDBytes"  # Quotes to prevent forward reference errors
    class Config:
        arbitrary_types_allowed = True  # Allows unrecognized types
import arraylake as al


def safe_reindex(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
    assert df1.index.isin(df2.index).all()
    return df1.reindex(df2.index)

def xr_pct_change(da: xr.DataArray, dim: str, periods: int = 1) -> xr.DataArray:
    """
    Calculate the percent change between values of an xarray DataArray along a specified dimension,
    similar to the pandas pct_change function.

    Parameters
    ----------
    da : xr.DataArray
        The input xarray DataArray.
    dim : str
        The dimension along which to calculate the percent change.
    periods : int, optional
        The number of periods to shift for calculating percent change, by default 1.

    Returns
    -------
    xr.DataArray
        DataArray of the same shape with percent changes computed along the specified dimension.

    Examples
    --------
    >>> da = xr.DataArray([1, 2, 4, 8], dims='time')
    >>> xr_pct_change(da, dim='time')
    <xarray.DataArray (time: 4)>
    array([nan, 1. , 1. , 1. ])
    Dimensions without coordinates: time

    Notes
    -----
    - Generated by ChatGPT (v2.0) on 2024-10-22.
    - Prompt: "Please write a pct_change function for an xarray DataArray in the style of pandas pct_change function.
      Please be careful to note that the diff function in xarray uses the numpy convention, not the pandas convention."
    """
    shifted = da.shift({dim: periods})
    pct_change = (da - shifted) / shifted
    # pct_change = pct_change.where(~isinf(pct_change))  # Handle division by zero cases
    return pct_change


def check_memory_usage():
    '''Returns memory usage in MB'''
    memory_info = psutil.Process().memory_info()
    return int(memory_info.rss / 1024 ** 2) + 1
    # print(f"Memory usage: {memory_info.rss / 1024 ** 2:.2f} MB")


def summarize_memory_usage():
    '''Summarizes memory in GB'''
    _dict = ({'Process usage':    psutil.Process().memory_info().rss,
              'System available': psutil.virtual_memory().available,
              'System total':     psutil.virtual_memory().total,
              })
    return (pd.Series(name ='Memory Usage (GB)',
                      data = _dict)
            .div(1024 ** 3)
            .round(3))


def write_pickle(obj: Any, path: str) -> None:
    with open(path, 'wb') as f:
        pickle.dump(obj, f, protocol=-1)

def read_pickle(path: str) -> Any:
    with open(path, 'rb') as f:
        return pickle.load(f)

def write_zarr(ds: xr.Dataset, path: str) -> None:
    ds.to_zarr(path, mode='w')

def read_zarr(path: str) -> xr.Dataset:
    return xr.open_zarr(path)

def read_file(path: str, type: str) -> Any:
    type_dict = {'pkl':  read_pickle, 
                 'zarr': read_zarr}
    return type_dict[type](path)    

def write_file(obj: Any, path: str, type: str) -> None:
    type_dict = {'pkl':  write_pickle, 
                 'zarr': write_zarr}
    return type_dict[type](obj, path)

def read_arraylake(repo_name: Optional[str] = None) -> xr.Dataset:
    if repo_name is None:
        repo_name = ARRAYLAKE_REPO
    client = al.Client()
    repo = client.get_repo(repo_name)
    session = repo.readonly_session("main")
    print(f"Reading from {repo_name}")
    return xr.open_zarr(session.store, consolidated=False)

def write_arraylake(ds: xr.Dataset, repo_name: Optional[str] = None,
                    commit_message: str = 'Update dataset',
                    append_dim='date') -> None:
    if repo_name is None:
        repo_name = ARRAYLAKE_REPO
    client = al.Client()
    repo = client.get_repo(repo_name)
    session = repo.writable_session("main")
    ds.to_zarr(session.store, mode='a', consolidated=False, append_dim=append_dim) #, chunks='auto')
    session.commit(commit_message)
    print(f"Committed {commit_message} to {repo_name}")


def business_days_ago(n=1):
    return (pd.Timestamp.today() - BDay(n)).date()


def _cache_to_disk(func, *args, read_cache=True, write_cache=True, cache_dir=CACHE_DIR, cache_file=None, check=None, file_type='zarr', **kwargs):
    if check is None:
        check = lambda x: True
    # TODO: Think about default cache file type. 
    #       Datasets should be zarr, others should be pkl
    #       Use func's type hint to determine object type?
    # if file_type is None: 
    #     if isinstance(args[0], xr.Dataset):
    #         file_type = 'zarr'
    #     else:
    #         file_type = 'pkl'
    if cache_file is None:
        cache_file = f'{func.__name__}.{file_type}'
    cache_path = os.path.join(cache_dir, cache_file)

    if read_cache and os.path.exists(cache_path):
        data = read_file(cache_path, file_type)
        if not check(data):
            data = func(*args, **kwargs)
            if write_cache:
                write_file(data, cache_path, file_type)
    else:
        data = func(*args, **kwargs)
        if write_cache:
            write_file(data, cache_path, file_type)
    return data


def _cache_to_arraylake(func, *args, read_cache=True, write_cache=True, repo_name=None, check=None, **kwargs):
    if repo_name is None:
        # raise ValueError("For arraylake caching, 'repo_name' must be provided.")
        repo_name = ARRAYLAKE_REPO
    if check is None:
        check = lambda x: True
    if read_cache:
        data = read_arraylake(repo_name)
        if not check(data):
            data = func(*args, **kwargs)
        if write_cache:
            write_arraylake(data, repo_name)
    else:
        data = func(*args, **kwargs)
        if write_cache:
            write_arraylake(data, repo_name)
    return data

# @functools.wraps
def cache(target: Literal['disk', 'arraylake', 'streamlit'] = 'disk') -> Callable:
    # TODO: Understand and document streamlit staleness logic
    """
    A unified decorator to cache function results based on the specified target.

    Parameters
    ----------
    target : str, optional
        The caching method to use. Can be 'disk', 'arraylake', or 'streamlit'. Defaults to 'disk'.
        - If 'disk', caches the result to a file (default: '.pkl' or '.zarr').
        - If 'arraylake', caches the result to an Arraylake repository (requires 'repo_name').
        - If 'streamlit', caches the result using Streamlit's caching system.

    Usage Instructions
    -------------------
    - For **file caching** ('disk'):
        - **`cache_file`**: Optional. Specifies the cache file name (defaults to the function name with `.pkl` or `.zarr` extension).
        - **`cache_dir`**: Optional. Specifies the cache directory (defaults to `'cache'`).
        - **`file_type`**: Optional. Specifies the file type (defaults to `'zarr'`, but can also be `'pkl'`).
    
    - For **arraylake caching** ('arraylake'):
        - **`repo_name`**: Required. Specifies the Arraylake repository where the data is cached.
    
    - For **Streamlit caching** ('streamlit'):
        - No extra parameters are required. The function is automatically cached using Streamlit's `st.cache_data`.



    Wrapper Parameters:
    -------------------
    - **`read_cache`**: bool, optional, default=True
        - **For 'disk' and 'arraylake' targets**: Attempt reading from the cache.
        - **For 'streamlit' target**: Ignored.
    
    - **`write_cache`**: bool, optional, default=True
        - **For 'disk' and 'arraylake' targets**: Write to cache if data was just read from cache.
        - **For 'streamlit' target**: Ignored.
    
    - **`cache_dir`**: str, optional, default='cache'
        - **For 'disk' target**: Specifies the directory where cache files are stored.
        - **For 'arraylake' and 'streamlit' target**: Ignored.
    
    - **`cache_file`**: str, optional
        - **For 'disk' target**: Specifies the name of the cache file (defaults to function name with `.zarr` or `.pkl` extension).
        - **For 'arraylake' and 'streamlit' target**: Ignored.
    
    - **`repo_name`**: str, optional
        - **For 'arraylake' target**: Specifies the Arraylake repository to store the cache.
        - **For 'disk' and 'streamlit' targets**: Ignored.
    
    - **`check`**: Callable, optional, default=None
        - **For 'disk' and 'arraylake' targets**: A function to validate whether the cached data is still valid. If `check(data)` returns False, the function is recomputed.
        - **For 'streamlit' target**: Ignored.
    
    - **`file_type`**: Literal['pkl', 'zarr'], optional, default='pkl'
        - **For 'disk' target**: Specifies the file type for the cache file (either 'pkl' or 'zarr').
        - **For 'arraylake' and 'streamlit' targets**: Ignored.
    
    - **`kwargs`**: List[Any], optional
        - Additional keyword arguments passed to the decorated function.

    """
    
    def decorator(func: Callable) -> Callable:
        def wrapper(*args:       List[Any], 
                    read_cache:  bool = True, 
                    write_cache: bool = True, 
                    cache_dir:   str = CACHE_DIR, 
                    cache_file:  Optional[str] = None, 
                    repo_name:   Optional[str] = None, 
                    check:       Optional[Callable] = None, 
                    file_type:   Literal['pkl', 'zarr'] = 'zarr', 
                    **kwargs:    List[Any]
                    ) -> Any:

            match target:
                case 'disk':
                    data = _cache_to_disk(func, *args, read_cache=read_cache, write_cache=write_cache, cache_dir=cache_dir, cache_file=cache_file, check=check, file_type=file_type, **kwargs)
                case 'arraylake':
                    data = _cache_to_arraylake(func, *args, read_cache=read_cache, write_cache=write_cache, repo_name=repo_name, check=check, **kwargs)
                case 'streamlit':
                    data = st.cache_data(func)(*args, **kwargs)
                    return data
                case _:
                    raise ValueError(f"Unknown cache target: {target}. Must be one of 'disk', 'arraylake', or 'streamlit'.")
            return data

        return wrapper
    return decorator


def align_indices(df1: pd.DataFrame, df2: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    Align the indices of two DataFrames to ensure they can be joined without errors.
    
    Parameters
    ----------
    df1 : pd.DataFrame
        The first DataFrame.
    df2 : pd.DataFrame
        The second DataFrame.
    
    Returns
    -------
    (pd.DataFrame, pd.DataFrame)
        The two DataFrames with aligned indices.
    """
    common_index = df1.index.union(df2.index)
    return df1.reindex(common_index), df2.reindex(common_index)

