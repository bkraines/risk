from datetime import date
from typing import Callable, Any, List, Literal, Optional, Union

import os 
from pathlib import Path
import datetime

from pandas.tseries.offsets import BDay
import psutil
import pickle

import pandas as pd
import xarray as xr
import streamlit as st

from risk_lib.config import CACHE_DIR, ARRAYLAKE_REPO
# from config import CACHE_DIR, ARRAYLAKE_REPO
# print("--- Loading risk_lib.util ---")



# Patch `pydantic`, used by `streamlit` server, which doesn't recognize `DBIDBytes` type from `arraylake`.
# Patch generated by ChatGPT
from pydantic import BaseModel
class MyModel(BaseModel):
    db_id: "DBIDBytes"  # Quotes to prevent forward reference errors
    class Config:
        arbitrary_types_allowed = True  # Allows unrecognized types
import arraylake as al


def safe_reindex(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
    assert df1.index.isin(df2.index).all()
    return df1.reindex(df2.index)


def to_pandas_strict(self) -> pd.DataFrame | pd.Series:
    """
    Monkey-patch replacing `to_pandas` that returns only pandas objects, DataFrame or Series.
    Prevents edge case where `to_pandas` returns a DataArray given a 0-dimensional DataArray.
    Useful for type hinting safety.
    
    Returns
    -------
        pd.DataFrame | pd.Series
    """    
    # TODO: Doesn't work!
    result = self.to_pandas()
    if isinstance(result, xr.DataArray):
        return result.to_series() if result.ndim == 1 else result.to_dataframe()
    return result


def xr_pct_change(da: xr.DataArray, dim: str, periods: int = 1) -> xr.DataArray:
    """
    Calculate the percent change between values of an xarray DataArray along a specified dimension,
    similar to the pandas pct_change function.

    Parameters
    ----------
    da : xr.DataArray
        The input xarray DataArray.
    dim : str
        The dimension along which to calculate the percent change.
    periods : int, optional
        The number of periods to shift for calculating percent change, by default 1.

    Returns
    -------
    xr.DataArray
        DataArray of the same shape with percent changes computed along the specified dimension.

    Examples
    --------
    >>> da = xr.DataArray([1, 2, 4, 8], dims='time')
    >>> xr_pct_change(da, dim='time')
    <xarray.DataArray (time: 4)>
    array([nan, 1. , 1. , 1. ])
    Dimensions without coordinates: time

    Notes
    -----
    - Generated by ChatGPT (v2.0) on 2024-10-22.
    - Prompt: "Please write a pct_change function for an xarray DataArray in the style of pandas pct_change function.
      Please be careful to note that the diff function in xarray uses the numpy convention, not the pandas convention."
    """
    shifted = da.shift({dim: periods})
    pct_change = (da - shifted) / shifted
    # pct_change = pct_change.where(~isinf(pct_change))  # Handle division by zero cases
    return pct_change


def check_memory_usage():
    '''Returns memory usage in MB'''
    memory_info = psutil.Process().memory_info()
    return int(memory_info.rss / 1024 ** 2) + 1
    # print(f"Memory usage: {memory_info.rss / 1024 ** 2:.2f} MB")


def summarize_memory_usage():
    '''Summarizes memory in GB'''
    _dict = ({'Process usage':    psutil.Process().memory_info().rss,
              'System available': psutil.virtual_memory().available,
              'System total':     psutil.virtual_memory().total,
              })
    return (pd.Series(name ='Memory Usage (GB)',
                      data = _dict)
            .div(1024 ** 3)
            .round(3))


def write_pickle(obj: Any, path: str) -> None:
    with open(path, 'wb') as f:
        pickle.dump(obj, f, protocol=-1)

def read_pickle(path: str) -> Any:
    with open(path, 'rb') as f:
        return pickle.load(f)

def write_zarr(ds: xr.Dataset, path: str) -> None:
    ds.to_zarr(path, mode='w')

def read_zarr(path: str) -> xr.Dataset:
    return xr.open_zarr(path)

def read_file(path: str, type: str) -> Any:
    type_dict = {'pkl':  read_pickle, 
                 'zarr': read_zarr}
    return type_dict[type](path)    

def write_file(obj: Any, path: str, type: str) -> None:
    type_dict = {'pkl':  write_pickle, 
                 'zarr': write_zarr}
    return type_dict[type](obj, path)

def read_arraylake(repo_name: Optional[str] = None) -> xr.Dataset:
    if repo_name is None:
        repo_name = ARRAYLAKE_REPO
    client = al.Client()
    repo = client.get_repo(repo_name)
    session = repo.readonly_session("main")
    print(f"Reading from {repo_name}")
    return xr.open_zarr(session.store, consolidated=False)

def write_arraylake(ds: xr.Dataset, repo_name: Optional[str] = None,
                    commit_message: str = 'Update dataset',
                    append_dim='date') -> None:
    if repo_name is None:
        repo_name = ARRAYLAKE_REPO
    client = al.Client()
    repo = client.get_repo(repo_name)
    session = repo.writable_session("main")
    ds.to_zarr(session.store, mode='a', consolidated=False, append_dim=append_dim) #, chunks='auto')
    session.commit(commit_message)
    print(f"Committed {commit_message} to {repo_name}")


def _cache_to_disk(func, *args, read_cache=True, write_cache=True, cache_dir=CACHE_DIR, cache_file=None, check=None, file_type='zarr', **kwargs):
    if check is None:
        check = lambda x: True
    # TODO: Think about default cache file type. 
    #       Datasets should be zarr, others should be pkl
    #       Use func's type hint to determine object type?
    # if file_type is None: 
    #     if isinstance(args[0], xr.Dataset):
    #         file_type = 'zarr'
    #     else:
    #         file_type = 'pkl'
    if cache_file is None:
        cache_file = f'{func.__name__}.{file_type}'
    cache_path = os.path.join(cache_dir, cache_file)

    if read_cache and os.path.exists(cache_path):
        data = read_file(cache_path, file_type)
        if not check(data):
            data = func(*args, **kwargs)
            if write_cache:
                write_file(data, cache_path, file_type)
    else:
        data = func(*args, **kwargs)
        if write_cache:
            write_file(data, cache_path, file_type)
    return data


def _cache_to_arraylake(func, *args, read_cache=True, write_cache=True, repo_name=None, check=None, **kwargs):
    if repo_name is None:
        # raise ValueError("For arraylake caching, 'repo_name' must be provided.")
        repo_name = ARRAYLAKE_REPO
    if check is None:
        check = lambda x: True
    if read_cache:
        data = read_arraylake(repo_name)
        if not check(data):
            data = func(*args, **kwargs)
        if write_cache:
            write_arraylake(data, repo_name)
    else:
        data = func(*args, **kwargs)
        if write_cache:
            write_arraylake(data, repo_name)
    return data

# @functools.wraps
def cache(target: Literal['disk', 'arraylake', 'streamlit'] = 'disk') -> Callable:
    # TODO: Understand and document streamlit staleness logic
    """
    A unified decorator to cache function results based on the specified target.

    Parameters
    ----------
    target : str, optional
        The caching method to use. Can be 'disk', 'arraylake', or 'streamlit'. Defaults to 'disk'.
        - If 'disk', caches the result to a file (default: '.pkl' or '.zarr').
        - If 'arraylake', caches the result to an Arraylake repository (requires 'repo_name').
        - If 'streamlit', caches the result using Streamlit's caching system.

    Usage Instructions
    -------------------
    - For **file caching** ('disk'):
        - **`cache_file`**: Optional. Specifies the cache file name (defaults to the function name with `.pkl` or `.zarr` extension).
        - **`cache_dir`**: Optional. Specifies the cache directory (defaults to `'cache'`).
        - **`file_type`**: Optional. Specifies the file type (defaults to `'zarr'`, but can also be `'pkl'`).
    
    - For **arraylake caching** ('arraylake'):
        - **`repo_name`**: Required. Specifies the Arraylake repository where the data is cached.
    
    - For **Streamlit caching** ('streamlit'):
        - No extra parameters are required. The function is automatically cached using Streamlit's `st.cache_data`.



    Wrapper Parameters:
    -------------------
    - **`read_cache`**: bool, optional, default=True
        - **For 'disk' and 'arraylake' targets**: Attempt reading from the cache.
        - **For 'streamlit' target**: Ignored.
    
    - **`write_cache`**: bool, optional, default=True
        - **For 'disk' and 'arraylake' targets**: Write to cache if data was just read from cache.
        - **For 'streamlit' target**: Ignored.
    
    - **`cache_dir`**: str, optional, default='cache'
        - **For 'disk' target**: Specifies the directory where cache files are stored.
        - **For 'arraylake' and 'streamlit' target**: Ignored.
    
    - **`cache_file`**: str, optional
        - **For 'disk' target**: Specifies the name of the cache file (defaults to function name with `.zarr` or `.pkl` extension).
        - **For 'arraylake' and 'streamlit' target**: Ignored.
    
    - **`repo_name`**: str, optional
        - **For 'arraylake' target**: Specifies the Arraylake repository to store the cache.
        - **For 'disk' and 'streamlit' targets**: Ignored.
    
    - **`check`**: Callable, optional, default=None
        - **For 'disk' and 'arraylake' targets**: A function to validate whether the cached data is still valid. If `check(data)` returns False, the function is recomputed.
        - **For 'streamlit' target**: Ignored.
    
    - **`file_type`**: Literal['pkl', 'zarr'], optional, default='pkl'
        - **For 'disk' target**: Specifies the file type for the cache file (either 'pkl' or 'zarr').
        - **For 'arraylake' and 'streamlit' targets**: Ignored.
    
    - **`kwargs`**: List[Any], optional
        - Additional keyword arguments passed to the decorated function.

    """
    
    def decorator(func: Callable) -> Callable:
        def wrapper(*args:       List[Any], 
                    read_cache:  bool = True, 
                    write_cache: bool = True, 
                    cache_dir:   str = CACHE_DIR, 
                    cache_file:  Optional[str] = None, 
                    repo_name:   Optional[str] = None, 
                    check:       Optional[Callable] = None, 
                    file_type:   Literal['pkl', 'zarr'] = 'zarr', 
                    **kwargs:    List[Any]
                    ) -> Any:

            match target:
                case 'disk':
                    data = _cache_to_disk(func, *args, read_cache=read_cache, write_cache=write_cache, cache_dir=cache_dir, cache_file=cache_file, check=check, file_type=file_type, **kwargs)
                case 'arraylake':
                    data = _cache_to_arraylake(func, *args, read_cache=read_cache, write_cache=write_cache, repo_name=repo_name, check=check, **kwargs)
                case 'streamlit':
                    data = st.cache_data(func)(*args, **kwargs)
                    return data
                case _:
                    raise ValueError(f"Unknown cache target: {target}. Must be one of 'disk', 'arraylake', or 'streamlit'.")
            return data

        return wrapper
    return decorator


def align_indices(df1: pd.DataFrame, df2: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    Align the indices of two DataFrames to ensure they can be joined without errors.
    
    Parameters
    ----------
    df1 : pd.DataFrame
        The first DataFrame.
    df2 : pd.DataFrame
        The second DataFrame.
    
    Returns
    -------
    (pd.DataFrame, pd.DataFrame)
        The two DataFrames with aligned indices.
    """
    common_index = df1.index.union(df2.index)
    return df1.reindex(common_index), df2.reindex(common_index)


def move_columns_to_front(df: pd.DataFrame, column_names: list[str]) -> pd.DataFrame:
    """
    Reorder a DataFrame so that the specified columns are moved to the front.

    Parameters
    ----------
    df : pd.DataFrame
        The original DataFrame.
    columns_names : list of str
        List of column names to move to the front. Must be a subset of df.columns.

    Returns
    -------
    pd.DataFrame
        A new DataFrame with the specified columns at the front.
    """
    remaining_cols = [col for col in df.columns if col not in column_names]
    return df[column_names + remaining_cols]


# def find_prior_month_end(dates: list[pd.Timestamp], current_date: Optional[pd.Timestamp]) -> pd.Timestamp:
#     dates = sorted(pd.to_datetime(dates))
#     if current_date is None:
#         current_date = dates[-1]
#     # current_date = pd.to_datetime(current_date)
#     prior_month_dates = dates[dates < current_date.replace(day=1)]
#     if not prior_month_dates.empty:
#         return prior_month_dates.max()
#     else:
#         return dates.min()  # First date in the sorted list


def get_mtd_range(today: Optional[date] = None) -> tuple[date, date]:
    """
    Return the month-to-date range from the first of the current month to today.

    Parameters
    ----------
    today : date, optional
        The reference date. Defaults to today's date.

    Returns
    -------
    tuple of date
        (start_date, end_date)
    """
    # TODO: Return the last date of the prior month from a list
    today = today or date.today()
    start = date(today.year, today.month, 1)
    return start, today


def get_ytd_range(today: Optional[date] = None) -> tuple[date, date]:
    """
    Return the year-to-date range from Jan 1 to today.

    Parameters
    ----------
    today : date, optional
        The reference date. Defaults to today's date.

    Returns
    -------
    tuple of date
        (start_date, end_date)
    """
    today = today or date.today()
    start = date(today.year, 1, 1)
    return start, today


# def select_date_range_bad(
#     date_list: Iterable[date],
#     trailing_windows: Optional[dict[str, int]] = None,
#     named_ranges: Optional[dict[str, tuple[date, date]]] = None
# ) -> tuple[date, date]:
#     """
#     Display a date range selector using Streamlit widgets.

#     Parameters
#     ----------
#     date_list : iterable of date
#         Available dates to select from.
#     trailing_windows : dict of str to int, optional
#         Mapping from label to number of days before the end date. Defaults to TRAILING_WINDOWS.
#     named_ranges : dict of str to tuple(date, date), optional
#         Mapping from label to explicit date ranges. Defaults to MARKET_EVENTS.

#     Returns
#     -------
#     tuple of date
#         The selected start and end dates.
#     """
#     if trailing_windows is None:
#             traling_windows = TRAILING_WINDOWS
#     if named_ranges is None:
#             named_ranges = MARKET_EVENTS

#     today = date_list[-1]

#     def get_trailing_date(n: int) -> date:
#         index = max(0, len(date_list) - n - 1)
#         return date_list[index]

#     trailing_options = {
#         label: (get_trailing_date(n), today)
#         for label, n in trailing_windows.items()
#     }

#     to_date_windows = {
#         # TODO: Replace with get_prior_month_end(date_list), get_prior_year_end(date_list)
#         "MTD": get_mtd_range(today),
#         "YTD": get_ytd_range(today),
#     }

#     static_options = {"max": (date_list[0], today), "custom": (None, None)}

#     all_date_options = {
#         **static_options,
#         **to_date_windows,
#         **trailing_options,
#         **named_ranges,
#     }

#     default_option = "max" if "max" in all_date_options else list(all_date_options.keys())[0]
#     label = st.selectbox("Date Range", options=list(all_date_options.keys()), index=list(all_date_options.keys()).index(default_option))

#     start_date, end_date = all_date_options[label]

#     if label == "custom":
#         start_date = st.date_input("Start date", value=today, max_value=today)
#         end_date = st.date_input("End date", value=today, min_value=start_date, max_value=today)

#         if start_date > end_date:
#             st.error("Start date must be before end date.")

#     st.caption(f"{start_date} â†’ {end_date}")
#     return start_date, end_date


def format_date(date_str):
    return pd.to_datetime(date_str).strftime(r'%m/%d')


def business_days_ago(n=1, current_date=None):
    if current_date is None:
        current_date = pd.Timestamp.today()
    return (pd.Timestamp.today() - BDay(n)).date()


# def current_business_day(current_date=None):
#     if current_date is None:
#         current_date = pd.Timestamp.today()
#     return (pd.Timestamp.today().date() + BDay(1) - BDay(1)).date()

def latest_business_day(date=None):
    if date is None:
        date = pd.Timestamp.today()
    return (date + BDay(1) - BDay(1)).date()

# print("--- Defining ttt ---")
# def ttt():
#     pass
# print("--- Finished loading risk_lib.util ---")

def get_directory_last_updated_time(path: str | Path) -> datetime.datetime:
    """
    Get the most recent modification time from all files in a directory and its subdirectories.

    Parameters
    ----------
    path : str or Path
        Path to the root of the directory.

    Returns
    -------
    datetime.datetime
        Timestamp of the most recently modified file in the directory or any of its subdirectories.
    """
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"Path does not exist: {path}")

    latest_mtime = max(
        f.stat().st_mtime
        for f in path.rglob("*")
        if f.is_file()
    )

    return datetime.datetime.fromtimestamp(latest_mtime)
