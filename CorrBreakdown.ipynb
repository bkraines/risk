{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xe8aT-ZG3aK"
      },
      "source": [
        "## Methodology: Portfolio-Weighted Mahalanobis Distance and Shock Decomposition (Covariance Matrix Formulation)\n",
        "\n",
        "This analysis assesses market shocks from the perspective of a specific portfolio. We define a portfolio-weighted Mahalanobis distance using raw asset returns ($\\mathbf{R}_t$) and the historical EWMA covariance matrix ($\\mathbf{\\Sigma}_{t-1}$), incorporating explicit portfolio weights ($\\mathbf{W}$). This distance is then decomposed into a \"weighted volatility shock component\" (representing the shock if assets were uncorrelated, viewed through portfolio weights) and a \"weighted correlation shock component.\" A separate section will detail how the correlation component links back to the underlying correlation matrix ($\\mathbf{\\Lambda}_{t-1}$) and standardized returns ($\\mathbf{z}_t$) for enhanced interpretability.\n",
        "\n",
        "### 1. Data Acquisition and Portfolio Definition\n",
        "\n",
        "* **Asset Selection**: A portfolio of $N$ assets is defined.\n",
        "* **Portfolio Weights ($\\mathbf{w}$)**: A vector of portfolio weights $\\mathbf{w} = [w_1, w_2, \\dots, w_N]^T$ is defined, where $\\sum_{i=1}^{N} w_i = 1$. Let $\\mathbf{W}$ be an $N \\times N$ diagonal matrix with the weights $w_i$ on its diagonal.\n",
        "* **Data Download**: Historical daily closing prices ($P_{i,t}$) are downloaded.\n",
        "* **Return Calculation**: Daily logarithmic returns $\\mathbf{R}_t = [R_{1,t}, \\dots, R_{N,t}]^T$ are calculated:\n",
        "$$\n",
        "R_{i,t} = \\ln\\left(\\frac{P_{i,t}}{P_{i,t-1}}\\right)\n",
        "$$\n",
        "\n",
        "### 2. EWMA Parameter Estimation (for day $t-1$)\n",
        "\n",
        "An Exponentially Weighted Moving Average (EWMA) approach is used for parameters based on data up to day $t-1$.\n",
        "\n",
        "* **EWMA Covariance Matrix ($\\mathbf{\\Sigma}_{t-1}$)**: The $N \\times N$ EWMA covariance matrix of asset returns.\n",
        "* **EWMA Variances ($\\sigma_{i,t-1}^2$)**: The EWMA variance for each asset $i$ is $\\sigma_{i,t-1}^2 = (\\mathbf{\\Sigma}_{t-1})_{ii}$. Let $\\mathbf{D}_{t-1}^2$ be the $N \\times N$ diagonal matrix of these variances (i.e., $\\mathbf{D}_{t-1}^2 = \\text{diag}(\\mathbf{\\Sigma}_{t-1})$). Thus, $\\mathbf{D}_{t-1}^{-2}$ is a diagonal matrix with $1/\\sigma_{i,t-1}^2$ on its diagonal. The matrix of standard deviations is $\\mathbf{D}_{t-1} = \\sqrt{\\mathbf{D}_{t-1}^2}$.\n",
        "\n",
        "### 3. Portfolio-Weighted Mahalanobis Distance ($M_{w,t}^2$)\n",
        "\n",
        "For each day $t$, assuming mean daily returns $\\boldsymbol{\\mu} \\approx \\mathbf{0}$:\n",
        "\n",
        "* **Total Portfolio-Weighted Mahalanobis Distance ($M_{w,t}^2$)**: This metric measures the \"unusualness\" of the raw asset returns $\\mathbf{R}_t$ when viewed through the lens of the portfolio weights $\\mathbf{W}$ and scaled by the full historical covariance structure $\\mathbf{\\Sigma}_{t-1}$. It is defined as:\n",
        "$$\n",
        "M_{w,t}^2 = \\mathbf{R}_t^T \\mathbf{W} \\mathbf{\\Sigma}_{t-1}^{-1} \\mathbf{W} \\mathbf{R}_t\n",
        "$$\n",
        "A high value of $M_{w,t}^2$ indicates that the observed joint returns, as weighted by the portfolio structure, represent a significant deviation from historical patterns of volatility and co-movement.\n",
        "\n",
        "### 4. Decomposition of Portfolio-Weighted Mahalanobis Distance ($M_{w,t}^2$)\n",
        "\n",
        "The total weighted shock $M_{w,t}^2$ can be decomposed as follows:\n",
        "\n",
        "<ul>\n",
        "    <li><strong>Weighted Volatility Shock Component ($VS_{w,t}$):</strong>\n",
        "        <ul>\n",
        "            <li><strong>Intuition</strong>: This component represents the portion of the total shock that can be attributed to the magnitude of individual asset returns, scaled by their portfolio weights and individual historical volatilities, <em>as if the assets were uncorrelated</em>.</li>\n",
        "            <li><strong>Formulation</strong>: It is the portfolio-weighted Mahalanobis distance calculated using only the diagonal part of the covariance matrix ($\\mathbf{D}_{t-1}^2$, representing variances only and zero covariances):\n",
        "            $$\n",
        "            VS_{w,t} = \\mathbf{R}_t^T \\mathbf{W} (\\mathbf{D}_{t-1}^2)^{-1} \\mathbf{W} \\mathbf{R}_t = \\mathbf{R}_t^T \\mathbf{W} \\mathbf{D}_{t-1}^{-2} \\mathbf{W} \\mathbf{R}_t\n",
        "            $$\n",
        "            </li>\n",
        "            <li><strong>Interpretation</strong>: Expanding this, $VS_{w,t} = \\sum_{i=1}^{N} w_i^2 \\left(\\frac{R_{i,t}}{\\sigma_{i,t-1}}\\right)^2$. This shows it represents the sum of squared portfolio-weighted standardized returns. A large $VS_{w,t}$ indicates that significant contributions to the shock came from large individual asset returns relative to their volatilities, amplified by their squared portfolio weights.</li>\n",
        "        </ul>\n",
        "    </li>\n",
        "    <li><strong>Weighted Correlation Shock Component ($CS_{w,t}$):</strong>\n",
        "        <ul>\n",
        "            <li><strong>Intuition</strong>: This component quantifies the portion of the total shock that arises specifically because asset returns <em>are</em> correlated (i.e., their covariances are non-zero), rather than moving independently. It measures the impact of the historical inter-asset co-movements (captured by the off-diagonal elements of $\\mathbf{\\Sigma}_{t-1}$) on the total shock, beyond what individual variances explain.</li>\n",
        "            <li><strong>Formulation</strong>: It is the difference between the total weighted Mahalanobis distance (which uses the full covariance matrix $\\mathbf{\\Sigma}_{t-1}$) and the weighted volatility shock component (which effectively assumes zero covariances):\n",
        "            $$\n",
        "            CS_{w,t} = M_{w,t}^2 - VS_{w,t}\n",
        "            $$\n",
        "            Substituting the expressions:\n",
        "            $$\n",
        "            CS_{w,t} = \\mathbf{R}_t^T \\mathbf{W} \\mathbf{\\Sigma}_{t-1}^{-1} \\mathbf{W} \\mathbf{R}_t - \\mathbf{R}_t^T \\mathbf{W} \\mathbf{D}_{t-1}^{-2} \\mathbf{W} \\mathbf{R}_t\n",
        "            $$\n",
        "            This can be factored as:\n",
        "            $$\n",
        "            CS_{w,t} = \\mathbf{R}_t^T \\mathbf{W} \\left(\\mathbf{\\Sigma}_{t-1}^{-1} - \\mathbf{D}_{t-1}^{-2}\\right) \\mathbf{W} \\mathbf{R}_t\n",
        "            $$\n",
        "            </li>\n",
        "            <li><strong>Brief Interpretation</strong>: A large positive $CS_{w,t}$ indicates that historical co-movements (correlations) amplified the overall portfolio-weighted shock. A large negative $CS_{w,t}$ suggests they dampened it. The detailed connection to the correlation matrix $\\mathbf{\\Lambda}_{t-1}$ and standardized returns is explored in Section 5.</li>\n",
        "        </ul>\n",
        "    </li>\n",
        "</ul>\n",
        "\n",
        "### 5. Interpreting the Weighted Correlation Shock Component ($CS_{w,t}$)\n",
        "\n",
        "<p>This section details the connection of the Weighted Correlation Shock Component ($CS_{w,t}$) to the underlying EWMA correlation matrix ($\\mathbf{\\Lambda}_{t-1}$) and standardized asset returns ($\\mathbf{z}_t$), providing a deeper understanding of its meaning.</p>\n",
        "<ul>\n",
        "    <li><strong>Key Result</strong>: The Weighted Correlation Shock Component, $CS_{w,t}$, initially defined in Section 4 using the covariance matrix $\\mathbf{\\Sigma}_{t-1}$ and raw returns $\\mathbf{R}_t$, can be equivalently expressed in terms of standardized returns ($\\mathbf{z}_t$), portfolio weights ($\\mathbf{W}$), and the inverse of the historical EWMA correlation matrix ($\\mathbf{\\Lambda}_{t-1}^{-1}$) as follows:\n",
        "    $$\n",
        "    CS_{w,t} = \\mathbf{z}_t^T \\mathbf{W} (\\mathbf{\\Lambda}_{t-1}^{-1} - \\mathbf{I}) \\mathbf{W} \\mathbf{z}_t\n",
        "    $$\n",
        "    In this form, $\\mathbf{z}_t = \\mathbf{D}_{t-1}^{-1}\\mathbf{R}_t$ are the standardized returns, $\\mathbf{W}$ is the diagonal matrix of portfolio weights, $\\mathbf{\\Lambda}_{t-1}$ is the EWMA correlation matrix, and $\\mathbf{I}$ is the identity matrix. This expression explicitly highlights how $CS_{w,t}$ is driven by the interaction of the historical correlation structure with portfolio-weighted standardized returns.\n",
        "    </li>\n",
        "    <li><strong>Derivation of the Equivalent Expression for $CS_{w,t}$</strong>:\n",
        "        <ul>\n",
        "            <li>We start with the definition of $CS_{w,t}$ from Section 4:\n",
        "            $$\n",
        "            CS_{w,t} = \\mathbf{R}_t^T \\mathbf{W} \\left(\\mathbf{\\Sigma}_{t-1}^{-1} - \\mathbf{D}_{t-1}^{-2}\\right) \\mathbf{W} \\mathbf{R}_t\n",
        "            $$\n",
        "            </li>\n",
        "            <li>The term $(\\mathbf{\\Sigma}_{t-1}^{-1} - \\mathbf{D}_{t-1}^{-2})$ isolates the impact of covariances. This term can be rewritten using the correlation matrix $\\mathbf{\\Lambda}_{t-1} = \\mathbf{D}_{t-1}^{-1}\\mathbf{\\Sigma}_{t-1}\\mathbf{D}_{t-1}^{-1}$ and the identity matrix $\\mathbf{I}$ via the identity:\n",
        "            $$\n",
        "            \\mathbf{\\Sigma}_{t-1}^{-1} - \\mathbf{D}_{t-1}^{-2} = \\mathbf{D}_{t-1}^{-1}(\\mathbf{\\Lambda}_{t-1}^{-1} - \\mathbf{I})\\mathbf{D}_{t-1}^{-1}\n",
        "            $$\n",
        "            </li>\n",
        "            <li>Substituting this identity into the expression for $CS_{w,t}$:\n",
        "            $$\n",
        "            CS_{w,t} = \\mathbf{R}_t^T \\mathbf{W} \\left[ \\mathbf{D}_{t-1}^{-1}(\\mathbf{\\Lambda}_{t-1}^{-1} - \\mathbf{I})\\mathbf{D}_{t-1}^{-1} \\right] \\mathbf{W} \\mathbf{R}_t\n",
        "            $$\n",
        "            </li>\n",
        "            <li>Next, we introduce standardized returns $\\mathbf{z}_t = \\mathbf{D}_{t-1}^{-1}\\mathbf{R}_t$, which implies $\\mathbf{R}_t = \\mathbf{D}_{t-1}\\mathbf{z}_t$. Since $\\mathbf{D}_{t-1}$ is diagonal, $\\mathbf{R}_t^T = \\mathbf{z}_t^T \\mathbf{D}_{t-1}$. Substituting these:\n",
        "            $$\n",
        "            CS_{w,t} = (\\mathbf{D}_{t-1}\\mathbf{z}_t)^T \\mathbf{W} \\mathbf{D}_{t-1}^{-1}(\\mathbf{\\Lambda}_{t-1}^{-1} - \\mathbf{I})\\mathbf{D}_{t-1}^{-1} \\mathbf{W} (\\mathbf{D}_{t-1}\\mathbf{z}_t)\n",
        "            $$\n",
        "            </li>\n",
        "            <li>This expression simplifies to:\n",
        "            $$\n",
        "            CS_{w,t} = \\mathbf{z}_t^T (\\mathbf{D}_{t-1} \\mathbf{W} \\mathbf{D}_{t-1}^{-1}) (\\mathbf{\\Lambda}_{t-1}^{-1} - \\mathbf{I}) (\\mathbf{D}_{t-1}^{-1} \\mathbf{W} \\mathbf{D}_{t-1}) \\mathbf{z}_t\n",
        "            $$\n",
        "            </li>\n",
        "            <li>Given that both $\\mathbf{D}_{t-1}$ (diagonal matrix of $\\sigma_{i,t-1}$) and $\\mathbf{W}$ (diagonal matrix of $w_i$) are diagonal matrices, the products involving them simplify: $(\\mathbf{D}_{t-1} \\mathbf{W} \\mathbf{D}_{t-1}^{-1}) = \\mathbf{W}$ and $(\\mathbf{D}_{t-1}^{-1} \\mathbf{W} \\mathbf{D}_{t-1}) = \\mathbf{W}$.</li>\n",
        "            <li>Applying these simplifications leads to the final equivalent expression stated in the key result:\n",
        "            $$\n",
        "            CS_{w,t} = \\mathbf{z}_t^T \\mathbf{W} (\\mathbf{\\Lambda}_{t-1}^{-1} - \\mathbf{I}) \\mathbf{W} \\mathbf{z}_t\n",
        "            $$\n",
        "            </li>\n",
        "        </ul>\n",
        "    </li>\n",
        "    <li><strong>Concluding Interpretation</strong>: This derived form explicitly shows that the Weighted Correlation Shock Component $CS_{w,t}$ quantifies how deviations of the historical correlation structure (represented by $\\mathbf{\\Lambda}_{t-1}^{-1}$) from an uncorrelated structure (represented by $\\mathbf{I}$) interact with the portfolio-weighted standardized returns ($\\mathbf{W}\\mathbf{z}_t$).</li>\n",
        "</ul>\n",
        "<p>By plotting $M_{w,t}^2$, $VS_{w,t}$, and $CS_{w,t}$ over time, we analyze portfolio-specific shocks, separating the impact of weighted individual asset variances from the impact of their historical co-movements as captured by the covariance structure. The interpretation of $CS_{w,t}$ is enriched by understanding its direct link to the correlation matrix $\\mathbf{\\Lambda}_{t-1}$ and standardized returns.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzHpEqrsG3KC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A59FgczjO2B"
      },
      "source": [
        "* **Add ptfl weights**\n",
        "* **Add Shapley Value**\n",
        "https://en.wikipedia.org/wiki/Shapley_value#:~:text=Shapley%20value%20regression%20is%20a,predictive%20power%20of%20the%20model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38jMHCOcjNwN"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# --- Configuration ---\n",
        "tickers = ['XLE', 'XLF', 'XLU', 'XLI', 'XLK', 'XLV', 'XLY', 'XLP', 'XLB']\n",
        "start_date = '2004-01-01'\n",
        "# Use a fixed end date for reproducibility if sharing output, or today for latest data\n",
        "# end_date = pd.Timestamp.today().strftime('%Y-%m-%d')\n",
        "end_date = '2025-05-27' # Using the date from your last output for consistency\n",
        "\n",
        "HALFLIFE = 126 # Approximately 6 months (trading days)\n",
        "MIN_PERIODS_COV = HALFLIFE # Minimum periods for EWMA covariance matrix\n",
        "\n",
        "# --- Portfolio Weights ---\n",
        "N_ASSETS_from_tickers = len(tickers)\n",
        "# For this example, using an equally weighted portfolio.\n",
        "w_portfolio_array = np.array([1/N_ASSETS_from_tickers] * N_ASSETS_from_tickers)\n",
        "W_diag_matrix = np.diag(w_portfolio_array) # This is \\mathbf{W}\n",
        "\n",
        "# --- !! DIAGNOSTIC CONTROL !! ---\n",
        "VERBOSE_DEBUG_LOOP = False # Keep False for cleaner summary output, True for deep dive\n",
        "PRINT_SERIES_DEBUG = True # Set to False if output is too long, True for debugging NaN issues\n",
        "\n",
        "# --- 1. Download Data ---\n",
        "data = pd.DataFrame()\n",
        "print(\"--- Data Acquisition and Initial Processing ---\")\n",
        "try:\n",
        "    print(f\"Attempting to download data for: {', '.join(tickers)}\")\n",
        "    print(f\"Date range: {start_date} to {end_date}\")\n",
        "    _raw_data = yf.download(tickers, start=start_date, end=end_date, progress=False)\n",
        "\n",
        "    if _raw_data.empty:\n",
        "        raise ValueError(\"yf.download returned an empty DataFrame.\")\n",
        "\n",
        "    if isinstance(_raw_data.columns, pd.MultiIndex):\n",
        "        data = _raw_data.xs('Close', level=0, axis=1)\n",
        "    elif len(tickers) == 1 and 'Close' in _raw_data.columns:\n",
        "        data = _raw_data[['Close']]\n",
        "        data.columns = tickers\n",
        "    elif all(ticker in _raw_data.columns for ticker in tickers) and len(tickers) == len(_raw_data.columns):\n",
        "        data = _raw_data\n",
        "    else:\n",
        "        print(\"Downloaded data columns structure is unexpected. Raw columns:\")\n",
        "        print(_raw_data.columns)\n",
        "        raise ValueError(\"Could not reliably extract 'Close' prices.\")\n",
        "\n",
        "    if data.empty or data.isnull().all().all():\n",
        "        raise ValueError(\"Extracted 'Close' price data is empty or all NaN.\")\n",
        "\n",
        "    data = data[tickers]\n",
        "    initial_rows_before_dropna = len(data)\n",
        "    data = data.dropna(axis=0, how='any')\n",
        "    print(f\"Data after dropna(axis=0, how='any'). Shape: {data.shape}. Rows dropped: {initial_rows_before_dropna - len(data)}\")\n",
        "\n",
        "    if data.shape[0] < MIN_PERIODS_COV + 5:\n",
        "         raise ValueError(f\"Insufficient data rows ({data.shape[0]}) after cleaning for EWMA. Need at least {MIN_PERIODS_COV + 5} rows.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL ERROR during data acquisition or initial processing: {e}\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Data successfully processed. Final price data usable: from {data.index.min().strftime('%Y-%m-%d')} to {data.index.max().strftime('%Y-%m-%d')}\")\n",
        "print(f\"Shape of final processed price data: {data.shape}\")\n",
        "\n",
        "# --- 2. Calculate Returns ---\n",
        "print(\"\\n--- Return Calculation ---\")\n",
        "returns = np.log(data / data.shift(1)).dropna()\n",
        "print(f\"Shape of returns data: {returns.shape}\")\n",
        "print(f\"Returns data from {returns.index.min().strftime('%Y-%m-%d')} to {returns.index.max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "if returns.shape[0] < MIN_PERIODS_COV + 1:\n",
        "    print(f\"CRITICAL ERROR: Insufficient returns data for analysis (have {returns.shape[0]} days, need at least {MIN_PERIODS_COV + 1} days). Exiting.\")\n",
        "    exit()\n",
        "\n",
        "N_ASSETS = returns.shape[1]\n",
        "if N_ASSETS != len(w_portfolio_array):\n",
        "    print(f\"CRITICAL ERROR: Mismatch between number of assets from data ({N_ASSETS}) and portfolio weights length ({len(w_portfolio_array)}).\")\n",
        "    exit()\n",
        "print(f\"Number of assets (N_ASSETS): {N_ASSETS}\")\n",
        "\n",
        "# --- 3. EWMA Covariance Series Calculation ---\n",
        "print(\"\\n--- EWMA Covariance Series Calculation ---\")\n",
        "print(f\"Calculating EWMA covariance series with halflife={HALFLIFE}, min_periods={MIN_PERIODS_COV}\")\n",
        "ewma_cov_full_series = returns.ewm(halflife=HALFLIFE, min_periods=MIN_PERIODS_COV).cov()\n",
        "if ewma_cov_full_series.dropna(how='all').empty:\n",
        "    print(\"CRITICAL ERROR: EWMA covariance series is entirely NaN after calculation. Check min_periods and data quality.\")\n",
        "    exit()\n",
        "valid_ewma_cov_dates = ewma_cov_full_series.index.get_level_values(0).unique()\n",
        "if valid_ewma_cov_dates.empty:\n",
        "    print(\"CRITICAL ERROR: No valid dates found in EWMA covariance series index.\")\n",
        "    exit()\n",
        "first_valid_cov_date_in_series = valid_ewma_cov_dates.min()\n",
        "print(f\"First date with non-NaN EWMA covariance data in series: {first_valid_cov_date_in_series.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "\n",
        "# --- Main Calculation Loop ---\n",
        "print(\"\\n--- Main Calculation Loop for Portfolio-Weighted Mahalanobis Distance & Components ---\")\n",
        "mah_dist_weighted_sq_rigorous = pd.Series(index=returns.index, dtype=float, name=\"M2_Weighted_Rigorous\")\n",
        "vol_shock_weighted_contrib_rigorous = pd.Series(index=returns.index, dtype=float, name=\"VS_Weighted_Rigorous\")\n",
        "corr_shock_weighted_contrib_rigorous = pd.Series(index=returns.index, dtype=float, name=\"CS_Weighted_Rigorous\")\n",
        "\n",
        "processed_days_count = 0\n",
        "skipped_days_summary = {\n",
        "    \"key_error_cov_lookup\": 0, \"type_error_sigma_lookup\":0, \"other_exc_sigma_lookup\": 0,\n",
        "    \"nan_inf_Sigma_t_minus_1\": 0, \"non_pos_variance\": 0,\n",
        "    \"ill_conditioned_Sigma\": 0, \"singular_Sigma_inversion\": 0, \"nan_inf_M2_result\": 0\n",
        "}\n",
        "total_loop_iterations = 0\n",
        "loop_start_index_val = 0\n",
        "try:\n",
        "    loop_start_date_for_t = returns.index[returns.index.get_loc(first_valid_cov_date_in_series) + 1]\n",
        "    loop_start_index_val = returns.index.get_loc(loop_start_date_for_t)\n",
        "except KeyError:\n",
        "    print(f\"CRITICAL ERROR: First valid EWMA cov date {first_valid_cov_date_in_series} or next day not in returns index. Alignment issue.\")\n",
        "    exit()\n",
        "\n",
        "if len(returns.index) > loop_start_index_val :\n",
        "    print(f\"Looping from returns.index position {loop_start_index_val} to {len(returns.index) - 1}.\")\n",
        "\n",
        "    for i in range(loop_start_index_val, len(returns.index)):\n",
        "        total_loop_iterations += 1\n",
        "        t_date = returns.index[i]\n",
        "        t_minus_1_date = returns.index[i-1]\n",
        "\n",
        "        if VERBOSE_DEBUG_LOOP and total_loop_iterations % 250 == 0 :\n",
        "            print(f\"  Verbose: Processing t_date: {t_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        R_t_vector = returns.loc[t_date].values.reshape(N_ASSETS, 1) # Ensure column vector\n",
        "\n",
        "        Sigma_t_minus_1 = np.array([])\n",
        "        try:\n",
        "            Sigma_t_minus_1_df_from_lookup = ewma_cov_full_series.loc[t_minus_1_date]\n",
        "            if not isinstance(Sigma_t_minus_1_df_from_lookup, pd.DataFrame):\n",
        "                skipped_days_summary[\"type_error_sigma_lookup\"] += 1; continue\n",
        "            Sigma_t_minus_1 = Sigma_t_minus_1_df_from_lookup.reindex(index=tickers, columns=tickers).values\n",
        "        except KeyError:\n",
        "            skipped_days_summary[\"key_error_cov_lookup\"] += 1; continue\n",
        "        except Exception as e:\n",
        "            skipped_days_summary[\"other_exc_sigma_lookup\"] += 1; continue\n",
        "\n",
        "        if Sigma_t_minus_1.size == 0:\n",
        "             skipped_days_summary[\"other_exc_sigma_lookup\"] += 1; continue\n",
        "        if np.isnan(Sigma_t_minus_1).any() or np.isinf(Sigma_t_minus_1).any():\n",
        "            skipped_days_summary[\"nan_inf_Sigma_t_minus_1\"] += 1; continue\n",
        "\n",
        "        # Check condition number of Sigma_t_minus_1\n",
        "        cond_Sigma = np.linalg.cond(Sigma_t_minus_1)\n",
        "        if cond_Sigma > 1e10: # Threshold for ill-conditioning\n",
        "            if VERBOSE_DEBUG_LOOP: print(f\"  DEBUG: {t_date.strftime('%Y-%m-%d')}: Sigma_t-1 ill-conditioned (cond={cond_Sigma:.2e}). Skipping.\")\n",
        "            skipped_days_summary[\"ill_conditioned_Sigma\"] += 1\n",
        "            continue\n",
        "\n",
        "        # Invert Sigma_t_minus_1\n",
        "        Sigma_t_minus_1_inv = np.array([])\n",
        "        try:\n",
        "            Sigma_t_minus_1_inv = np.linalg.inv(Sigma_t_minus_1)\n",
        "        except np.linalg.LinAlgError:\n",
        "            if VERBOSE_DEBUG_LOOP: print(f\"  DEBUG: {t_date.strftime('%Y-%m-%d')}: Singular Sigma_t-1 (LinAlgError). Skipping.\")\n",
        "            skipped_days_summary[\"singular_Sigma_inversion\"] += 1\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            if VERBOSE_DEBUG_LOOP: print(f\"  DEBUG: {t_date.strftime('%Y-%m-%d')}: Exception during Sigma_t-1 inversion: {e}. Skipping.\")\n",
        "            skipped_days_summary[\"singular_Sigma_inversion\"] += 1\n",
        "            continue\n",
        "        if Sigma_t_minus_1_inv.size == 0:\n",
        "            skipped_days_summary[\"singular_Sigma_inversion\"] += 1; continue\n",
        "\n",
        "        # --- CALCULATIONS USING COVARIANCE MATRIX DIRECTLY ---\n",
        "        # M_w_t^2 = R_t^T @ W @ Sigma_inv @ W @ R_t\n",
        "        M_w_t_sq_val = (R_t_vector.T @ W_diag_matrix @ Sigma_t_minus_1_inv @ W_diag_matrix @ R_t_vector).item()\n",
        "\n",
        "        if np.isnan(M_w_t_sq_val) or np.isinf(M_w_t_sq_val):\n",
        "            if VERBOSE_DEBUG_LOOP: print(f\"  DEBUG: {t_date.strftime('%Y-%m-%d')}: M_w_t_sq_val is NaN or Inf. Skipping.\")\n",
        "            skipped_days_summary[\"nan_inf_M2_result\"] += 1\n",
        "            continue\n",
        "        mah_dist_weighted_sq_rigorous.loc[t_date] = M_w_t_sq_val\n",
        "\n",
        "        # Calculate D_t_minus_1_sq (diagonal matrix of variances from Sigma_t-1)\n",
        "        diag_Sigma_t_minus_1 = np.diag(Sigma_t_minus_1) # Variances sigma_i^2\n",
        "        if np.any(diag_Sigma_t_minus_1 <= 1e-12): # Check for non-positive variance again, crucial for D_inv\n",
        "            if VERBOSE_DEBUG_LOOP: print(f\"  DEBUG: {t_date.strftime('%Y-%m-%d')}: Non-positive variance in diag_Sigma_t-1 for D_inv. Skipping.\")\n",
        "            skipped_days_summary[\"non_pos_variance\"] += 1\n",
        "            continue\n",
        "        D_t_minus_1_sq_inv_diag_elements = 1.0 / diag_Sigma_t_minus_1 # These are 1/sigma_i^2\n",
        "        D_t_minus_1_sq_inv = np.diag(D_t_minus_1_sq_inv_diag_elements) # This is D_{t-1}^{-2}\n",
        "\n",
        "        # VS_w_t = R_t^T @ W @ D_t-1^-2 @ W @ R_t\n",
        "        VS_w_t_val = (R_t_vector.T @ W_diag_matrix @ D_t_minus_1_sq_inv @ W_diag_matrix @ R_t_vector).item()\n",
        "        vol_shock_weighted_contrib_rigorous.loc[t_date] = VS_w_t_val\n",
        "\n",
        "        # CS_w_t = M_w_t^2 - VS_w_t\n",
        "        CS_w_t_val = M_w_t_sq_val - VS_w_t_val\n",
        "        corr_shock_weighted_contrib_rigorous.loc[t_date] = CS_w_t_val\n",
        "        # --- END OF MODIFIED CALCULATIONS ---\n",
        "\n",
        "        processed_days_count += 1\n",
        "else:\n",
        "    print(f\"CRITICAL WARNING: Not enough data to run the main calculation loop. Returns length={len(returns.index)}, loop_start_index_val={loop_start_index_val}\")\n",
        "\n",
        "print(f\"\\nLoop finished. Total iterations attempted in loop: {total_loop_iterations}\")\n",
        "print(f\"Total days successfully processed and values stored: {processed_days_count}\")\n",
        "print(\"Summary of reasons for skipping days during calculation:\")\n",
        "for reason, count in skipped_days_summary.items():\n",
        "    if count > 0:\n",
        "        print(f\"  - Skipped due to '{reason}': {count} days\")\n",
        "\n",
        "print(\"\\n--- Rigorous Series before dropna() ---\")\n",
        "def print_series_info(series, name):\n",
        "    if not series.empty:\n",
        "        print(f\"{name}: Length={len(series)}, Non-NaNs={series.count()} (NaNs={series.isna().sum()})\")\n",
        "        if PRINT_SERIES_DEBUG and series.count() > 0 :\n",
        "            print(f\"  Head of {name} (non-NaN):\\n{series.dropna().head()}\")\n",
        "            print(f\"  Tail of {name} (non-NaN):\\n{series.dropna().tail()}\")\n",
        "        elif PRINT_SERIES_DEBUG:\n",
        "            print(f\"  {name} contains no non-NaN values to show head/tail.\")\n",
        "    else:\n",
        "        print(f\"{name} is empty.\")\n",
        "\n",
        "print_series_info(mah_dist_weighted_sq_rigorous, \"mah_dist_weighted_sq_rigorous\")\n",
        "print_series_info(vol_shock_weighted_contrib_rigorous, \"vol_shock_weighted_contrib_rigorous\")\n",
        "print_series_info(corr_shock_weighted_contrib_rigorous, \"corr_shock_weighted_contrib_rigorous\")\n",
        "\n",
        "print(\"\\n--- Final Data Preparation for Plotting ---\")\n",
        "mah_dist_sq_final = mah_dist_weighted_sq_rigorous.dropna()\n",
        "vol_shock_contrib_final = vol_shock_weighted_contrib_rigorous.dropna()\n",
        "corr_shock_contrib_final = corr_shock_weighted_contrib_rigorous.dropna()\n",
        "\n",
        "common_valid_index = pd.Index([])\n",
        "if not mah_dist_sq_final.empty:\n",
        "    temp_idx = mah_dist_sq_final.index\n",
        "    if not vol_shock_contrib_final.empty:\n",
        "        temp_idx = temp_idx.intersection(vol_shock_contrib_final.index)\n",
        "    if not corr_shock_contrib_final.empty:\n",
        "        temp_idx = temp_idx.intersection(corr_shock_contrib_final.index)\n",
        "    common_valid_index = temp_idx\n",
        "    print(f\"Length of common valid index for plotting: {len(common_valid_index)}\")\n",
        "else:\n",
        "    print(\"Weighted Mahalanobis distance series (M_w2_final) is empty after dropna. Cannot plot.\")\n",
        "\n",
        "if common_valid_index.empty:\n",
        "    print(\"\\nCRITICAL WARNING: No common dates for plotting after dropna(), or M_w2_final is empty.\")\n",
        "    if mah_dist_sq_final.empty: print(\"  Weighted Mahalanobis distance final series is empty.\")\n",
        "    if vol_shock_contrib_final.empty: print(\"  Weighted Volatility shock final series is empty.\")\n",
        "    if corr_shock_contrib_final.empty: print(\"  Weighted Correlation shock final series is empty.\")\n",
        "    exit()\n",
        "\n",
        "mah_dist_plot = mah_dist_sq_final.reindex(common_valid_index)\n",
        "vol_shock_plot = vol_shock_contrib_final.reindex(common_valid_index).fillna(0)\n",
        "corr_shock_plot = corr_shock_contrib_final.reindex(common_valid_index).fillna(0)\n",
        "\n",
        "print(f\"\\nFinal series lengths for plotting: M_w^2={len(mah_dist_plot)}, VS_w={len(vol_shock_plot)}, CS_w={len(corr_shock_plot)}\")\n",
        "\n",
        "if len(mah_dist_plot) == 0 :\n",
        "    print(\"\\nCRITICAL ERROR: No data to plot. Weighted Mahalanobis distance plot series is empty.\")\n",
        "    exit()\n",
        "\n",
        "# --- Plotting ---\n",
        "print(\"\\n--- Plotting Results (Portfolio-Weighted, Covariance Formulation) ---\")\n",
        "fig, axs = plt.subplots(3, 1, figsize=(15, 12), sharex=True)\n",
        "\n",
        "axs[0].plot(mah_dist_plot.index, mah_dist_plot, label=r'$M_{w,t}^2$', color='blue', linewidth=1)\n",
        "axs[0].set_title(f'Portfolio-Weighted Mahalanobis Distance ($M_{{w,t}}^2 = \\mathbf{{R}}_t^T \\mathbf{{W}} \\mathbf{{\\Sigma}}_{{t-1}}^{{-1}} \\mathbf{{W}} \\mathbf{{R}}_t$) - EWMA Half-life: {HALFLIFE} days')\n",
        "axs[0].set_ylabel(r'$M_{w,t}^2$')\n",
        "axs[0].grid(True, linestyle=':', alpha=0.7)\n",
        "if not mah_dist_plot.empty:\n",
        "    threshold_m2 = mah_dist_plot.quantile(0.99)\n",
        "    axs[0].axhline(threshold_m2, color='red', linestyle='--', linewidth=1, label=f'99th Percentile ({threshold_m2:.2f})')\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(vol_shock_plot.index, vol_shock_plot, label=r'$VS_{w,t}$', color='green', linewidth=1)\n",
        "axs[1].set_title(f'Weighted Volatility Shock ($VS_{{w,t}} = \\mathbf{{R}}_t^T \\mathbf{{W}} \\mathbf{{D}}_{{t-1}}^{{-2}} \\mathbf{{W}} \\mathbf{{R}}_t$)')\n",
        "axs[1].set_ylabel(r'$VS_{w,t}$')\n",
        "axs[1].grid(True, linestyle=':', alpha=0.7)\n",
        "if not vol_shock_plot.empty:\n",
        "    threshold_vs = vol_shock_plot.quantile(0.99)\n",
        "    axs[1].axhline(threshold_vs, color='red', linestyle='--', linewidth=1, label=f'99th Percentile ({threshold_vs:.2f})')\n",
        "axs[1].legend()\n",
        "\n",
        "axs[2].plot(corr_shock_plot.index, corr_shock_plot, label=r'$CS_{w,t}$', color='purple', linewidth=1)\n",
        "axs[2].set_title(f'Weighted Correlation Shock ($CS_{{w,t}} = M_{{w,t}}^2 - VS_{{w,t}}$)')\n",
        "axs[2].set_ylabel(r'$CS_{w,t}$')\n",
        "axs[2].grid(True, linestyle=':', alpha=0.7)\n",
        "if not corr_shock_plot.empty:\n",
        "    threshold_cs_upper = corr_shock_plot.quantile(0.99)\n",
        "    threshold_cs_lower = corr_shock_plot.quantile(0.01)\n",
        "    axs[2].axhline(threshold_cs_upper, color='red', linestyle='--', linewidth=1, label=f'99th Percentile ({threshold_cs_upper:.2f})')\n",
        "    axs[2].axhline(threshold_cs_lower, color='orange', linestyle='--', linewidth=1, label=f'1st Percentile ({threshold_cs_lower:.2f})')\n",
        "axs[2].legend()\n",
        "\n",
        "for ax in axs:\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "    ax.xaxis.set_major_locator(mdates.YearLocator(base=2))\n",
        "    ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=[1,7]))\n",
        "\n",
        "fig.autofmt_xdate()\n",
        "plt.xlabel('Date')\n",
        "plt.tight_layout()\n",
        "print(\"Displaying plot...\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Analysis Complete ---\")\n",
        "print(f\"Note: Calculations use lagged (t-1) EWMA covariance (Sigma_t-1) and variances (D_t-1^2) for evaluating returns R_t.\")\n",
        "print(f\"Portfolio weights used (w_portfolio_array): {w_portfolio_array}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7DbCZGkMcgTw",
        "outputId": "2f54c179-0f2c-4f32-cc71-af59b32bd30a"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "# from scipy.stats import chi2 # Removed as using empirical quantile\n",
        "\n",
        "# --- Configuration ---\n",
        "tickers = ['XLE', 'XLF', 'XLU', 'XLI', 'XLK', 'XLV', 'XLY', 'XLP', 'XLB']\n",
        "start_date = '2004-01-01'\n",
        "end_date = pd.Timestamp.today().strftime('%Y-%m-%d') # Default to today\n",
        "# end_date = '2025-05-27' # Or use a fixed end date for consistency\n",
        "\n",
        "HALFLIFE = 126 # Approximately 6 months (trading days)\n",
        "MIN_PERIODS_COV = HALFLIFE\n",
        "\n",
        "# --- Portfolio Weights ---\n",
        "N_ASSETS_from_tickers = len(tickers)\n",
        "w_portfolio_array = np.array([1/N_ASSETS_from_tickers] * N_ASSETS_from_tickers) # Equally weighted\n",
        "W_diag_matrix = np.diag(w_portfolio_array)\n",
        "\n",
        "# --- Attribution Analysis Configuration ---\n",
        "# For the detailed TABLE printout of top N shock days\n",
        "NUM_SHOCK_DAYS_FOR_TABLE_DETAIL = 3\n",
        "# For the new BAR CHART, we will plot latest day + up to 3 latest shock days\n",
        "NUM_RECENT_SHOCK_DAYS_FOR_BAR_CHART = 3\n",
        "\n",
        "TRAILING_WEEK_DAYS = 5\n",
        "TRAILING_MONTH_DAYS = 21\n",
        "SHOCK_EVENT_QUANTILE = 0.99 # Identify top 1% of Mahalanobis distances as shocks\n",
        "\n",
        "# --- !! DIAGNOSTIC CONTROL !! ---\n",
        "VERBOSE_DEBUG_LOOP = False\n",
        "PRINT_SERIES_DEBUG = False # Set to True for more detailed series printouts if debugging\n",
        "\n",
        "# --- 1. Download Data ---\n",
        "data = pd.DataFrame()\n",
        "print(\"--- Data Acquisition and Initial Processing ---\")\n",
        "try:\n",
        "    print(f\"Attempting to download data for: {', '.join(tickers)}\")\n",
        "    print(f\"Date range: {start_date} to {end_date}\")\n",
        "    _raw_data = yf.download(tickers, start=start_date, end=end_date, progress=False)\n",
        "\n",
        "    if _raw_data.empty:\n",
        "        raise ValueError(\"yf.download returned an empty DataFrame.\")\n",
        "\n",
        "    if isinstance(_raw_data.columns, pd.MultiIndex):\n",
        "        data = _raw_data.xs('Close', level=0, axis=1)\n",
        "    elif len(tickers) == 1 and 'Close' in _raw_data.columns:\n",
        "        data = _raw_data[['Close']]; data.columns = tickers\n",
        "    elif all(ticker in _raw_data.columns for ticker in tickers) and len(tickers) == len(_raw_data.columns):\n",
        "        data = _raw_data\n",
        "    else:\n",
        "        print(\"Downloaded data columns structure is unexpected. Raw columns:\")\n",
        "        print(_raw_data.columns)\n",
        "        raise ValueError(\"Could not reliably extract 'Close' prices.\")\n",
        "\n",
        "    if data.empty or data.isnull().all().all():\n",
        "        raise ValueError(\"Extracted 'Close' price data is empty or all NaN.\")\n",
        "\n",
        "    data = data[tickers]\n",
        "    initial_rows_before_dropna = len(data)\n",
        "    data = data.dropna(axis=0, how='any')\n",
        "    print(f\"Data after dropna. Shape: {data.shape}. Rows dropped: {initial_rows_before_dropna - len(data)}\")\n",
        "\n",
        "    if data.shape[0] < MIN_PERIODS_COV + 5:\n",
        "         raise ValueError(f\"Insufficient data rows ({data.shape[0]}) after cleaning. Need {MIN_PERIODS_COV + 5}.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL ERROR during data acquisition: {e}\"); exit()\n",
        "\n",
        "print(f\"Data successfully processed. Usable: {data.index.min().strftime('%Y-%m-%d')} to {data.index.max().strftime('%Y-%m-%d')}. Shape: {data.shape}\")\n",
        "\n",
        "# --- 2. Calculate Returns ---\n",
        "print(\"\\n--- Return Calculation ---\")\n",
        "returns = np.log(data / data.shift(1))\n",
        "num_obs_lost_to_shift = returns.iloc[:,0].isna().sum()\n",
        "returns = returns.dropna()\n",
        "print(f\"Log returns. Shape: {returns.shape}. Initial NaNs removed: {num_obs_lost_to_shift} day(s).\")\n",
        "print(f\"Returns data: {returns.index.min().strftime('%Y-%m-%d')} to {returns.index.max().strftime('%Y-%m-%d')}\")\n",
        "if returns.shape[0] < MIN_PERIODS_COV + 1: print(f\"CRITICAL ERROR: Insufficient returns data ({returns.shape[0]} days). Need {MIN_PERIODS_COV + 1}.\"); exit()\n",
        "\n",
        "N_ASSETS = returns.shape[1]\n",
        "if N_ASSETS != len(w_portfolio_array): print(f\"CRITICAL ERROR: Asset count mismatch ({N_ASSETS} vs {len(w_portfolio_array)} weights).\"); exit()\n",
        "print(f\"Number of assets (N_ASSETS): {N_ASSETS}\")\n",
        "\n",
        "# --- 3. EWMA Covariance Series Calculation ---\n",
        "print(\"\\n--- EWMA Covariance Series Calculation ---\")\n",
        "ewma_cov_full_series = returns.ewm(halflife=HALFLIFE, min_periods=MIN_PERIODS_COV).cov()\n",
        "if ewma_cov_full_series.dropna(how='all').empty: print(\"CRITICAL ERROR: EWMA covariance series is entirely NaN.\"); exit()\n",
        "valid_ewma_cov_dates = ewma_cov_full_series.index.get_level_values(0).unique()\n",
        "if valid_ewma_cov_dates.empty: print(\"CRITICAL ERROR: No valid dates in EWMA covariance series index.\"); exit()\n",
        "first_valid_cov_date_in_series = valid_ewma_cov_dates.min()\n",
        "print(f\"First date for $\\Sigma_{{t-1}}$ availability: {first_valid_cov_date_in_series.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# --- Main Calculation Loop ---\n",
        "print(\"\\n--- Main Calculation Loop for Metrics & Daily Contributions ---\")\n",
        "mah_dist_series = pd.Series(index=returns.index, dtype=float, name=\"MahalanobisDistance\")\n",
        "vol_shock_series = pd.Series(index=returns.index, dtype=float, name=\"VolatilityShock\")\n",
        "corr_shock_series = pd.Series(index=returns.index, dtype=float, name=\"CorrelationShock\")\n",
        "contrib_M_daily_df = pd.DataFrame(index=returns.index, columns=tickers, dtype=float)\n",
        "contrib_VS_daily_df = pd.DataFrame(index=returns.index, columns=tickers, dtype=float)\n",
        "contrib_CS_daily_df = pd.DataFrame(index=returns.index, columns=tickers, dtype=float)\n",
        "\n",
        "processed_days_count = 0; skipped_days_summary = {k:0 for k in [\"no_lagged_cov\",\"type_err_sigma\",\"oth_exc_sigma\",\"nan_inf_sigma\",\"nan_sigma_retrieved\",\"non_pos_var\",\"ill_cond_sigma\",\"sing_sigma_inv\",\"nan_inf_m\"]}\n",
        "total_loop_iterations = 0; last_successful_calc_date = None; loop_start_index_val = 0\n",
        "try:\n",
        "    first_t_minus_1_idx_loc = returns.index.get_loc(first_valid_cov_date_in_series)\n",
        "    loop_start_index_val = first_t_minus_1_idx_loc + 1\n",
        "    if loop_start_index_val >= len(returns.index): raise ValueError(\"Not enough data points after first valid covariance date for loop.\")\n",
        "    print(f\"INFO: EWMA warm-up uses returns up to {returns.index[loop_start_index_val-1].strftime('%Y-%m-%d')}.\")\n",
        "    print(f\"INFO: Calculations start for t_date = {returns.index[loop_start_index_val].strftime('%Y-%m-%d')}.\")\n",
        "except KeyError: print(f\"CRITICAL ERROR: Date alignment issue. {first_valid_cov_date_in_series} not in returns index.\"); exit()\n",
        "\n",
        "if len(returns.index) > loop_start_index_val :\n",
        "    print(f\"Looping {len(returns.index) - loop_start_index_val} iterations.\")\n",
        "    for i in range(loop_start_index_val, len(returns.index)):\n",
        "        total_loop_iterations += 1; t_date = returns.index[i]; t_minus_1_date = returns.index[i-1]\n",
        "        R_t_vector = returns.loc[t_date].values.reshape(N_ASSETS, 1)\n",
        "        Sigma_t_minus_1_current = np.array([])\n",
        "        try:\n",
        "            if t_minus_1_date not in valid_ewma_cov_dates: skipped_days_summary[\"no_lagged_cov_data\"] +=1; continue\n",
        "            Sigma_t_minus_1_df_from_lookup = ewma_cov_full_series.loc[t_minus_1_date]\n",
        "            if not isinstance(Sigma_t_minus_1_df_from_lookup, pd.DataFrame): skipped_days_summary[\"type_err_sigma\"] += 1; continue\n",
        "            if Sigma_t_minus_1_df_from_lookup.isnull().values.any(): skipped_days_summary[\"nan_sigma_retrieved\"] += 1; continue\n",
        "            Sigma_t_minus_1_current = Sigma_t_minus_1_df_from_lookup.reindex(index=tickers, columns=tickers).values\n",
        "        except KeyError: skipped_days_summary[\"key_error_cov_lookup\"] += 1; continue\n",
        "        except Exception: skipped_days_summary[\"oth_exc_sigma\"] += 1; continue\n",
        "        if Sigma_t_minus_1_current.size == 0: skipped_days_summary[\"oth_exc_sigma\"] += 1; continue\n",
        "        if np.isnan(Sigma_t_minus_1_current).any() or np.isinf(Sigma_t_minus_1_current).any(): skipped_days_summary[\"nan_inf_sigma\"] += 1; continue\n",
        "        cond_Sigma = np.linalg.cond(Sigma_t_minus_1_current);\n",
        "        if cond_Sigma > 1e12: skipped_days_summary[\"ill_cond_sigma\"] += 1; continue\n",
        "        try: Sigma_t_minus_1_inv_current = np.linalg.inv(Sigma_t_minus_1_current)\n",
        "        except Exception: skipped_days_summary[\"sing_sigma_inv\"] += 1; continue\n",
        "        if Sigma_t_minus_1_inv_current.size == 0: skipped_days_summary[\"sing_sigma_inv\"] += 1; continue\n",
        "        M_val = (R_t_vector.T @ W_diag_matrix @ Sigma_t_minus_1_inv_current @ W_diag_matrix @ R_t_vector).item()\n",
        "        if np.isnan(M_val) or np.isinf(M_val) or M_val < 0: skipped_days_summary[\"nan_inf_m\"] += 1; continue\n",
        "        mah_dist_series.loc[t_date] = M_val\n",
        "        diag_Sigma_t_minus_1_current = np.diag(Sigma_t_minus_1_current)\n",
        "        if np.any(diag_Sigma_t_minus_1_current <= 1e-12):\n",
        "            skipped_days_summary[\"non_pos_var\"] += 1; vol_shock_series.loc[t_date]=np.nan; corr_shock_series.loc[t_date]=np.nan\n",
        "            contrib_M_daily_df.loc[t_date]=np.full(N_ASSETS,np.nan); contrib_VS_daily_df.loc[t_date]=np.full(N_ASSETS,np.nan); contrib_CS_daily_df.loc[t_date]=np.full(N_ASSETS,np.nan)\n",
        "            last_successful_calc_date = t_date; processed_days_count +=1; continue\n",
        "        D_t_minus_1_sq_inv_current = np.diag(1.0 / diag_Sigma_t_minus_1_current)\n",
        "        VS_val = (R_t_vector.T @ W_diag_matrix @ D_t_minus_1_sq_inv_current @ W_diag_matrix @ R_t_vector).item()\n",
        "        if np.isnan(VS_val) or np.isinf(VS_val) or VS_val < 0: vol_shock_series.loc[t_date]=np.nan; corr_shock_series.loc[t_date]=np.nan\n",
        "        else: vol_shock_series.loc[t_date] = VS_val; CS_val = M_val - VS_val; corr_shock_series.loc[t_date] = CS_val\n",
        "        A_M_shock_day = W_diag_matrix @ Sigma_t_minus_1_inv_current @ W_diag_matrix; v_M_shock_day = A_M_shock_day @ R_t_vector\n",
        "        C_M_k_array = R_t_vector.flatten() * v_M_shock_day.flatten(); contrib_M_daily_df.loc[t_date] = C_M_k_array\n",
        "        std_devs_t_minus_1_current = np.sqrt(diag_Sigma_t_minus_1_current)\n",
        "        if np.any(std_devs_t_minus_1_current <= 1e-8): C_VS_k_array = np.full(N_ASSETS, np.nan)\n",
        "        else: C_VS_k_array = (w_portfolio_array**2) * ((R_t_vector.flatten() / std_devs_t_minus_1_current)**2)\n",
        "        contrib_VS_daily_df.loc[t_date] = C_VS_k_array\n",
        "        if not (np.isnan(C_M_k_array).any() or np.isnan(C_VS_k_array).any()): contrib_CS_daily_df.loc[t_date] = C_M_k_array - C_VS_k_array\n",
        "        else: contrib_CS_daily_df.loc[t_date] = np.full(N_ASSETS, np.nan)\n",
        "        last_successful_calc_date = t_date; processed_days_count += 1\n",
        "else: print(f\"CRITICAL WARNING: Loop condition not met.\")\n",
        "print(f\"\\nLoop finished. Iterations: {total_loop_iterations}. Successfully processed days: {processed_days_count}\")\n",
        "if last_successful_calc_date: print(f\"Last date calculations stored: {last_successful_calc_date.strftime('%Y-%m-%d')}\")\n",
        "else: print(\"No days successfully processed.\")\n",
        "for reason, count in skipped_days_summary.items():\n",
        "    if count > 0: print(f\"  - Skipped due to '{reason}': {count} days\")\n",
        "\n",
        "def print_series_info_debug(series, name):\n",
        "    if isinstance(series, pd.Series):\n",
        "        if PRINT_SERIES_DEBUG and not series.dropna().empty : print(f\"  Tail of {name} (non-NaN):\\n{series.dropna().tail()}\")\n",
        "    elif isinstance(series, pd.DataFrame):\n",
        "        if PRINT_SERIES_DEBUG and not series.dropna(how='all').empty: print(f\"  Tail of {name} (first col, non-NaN):\\n{series[series.columns[0]].dropna().tail()}\")\n",
        "print(\"\\n--- Populated Series (Tail Samples if PRINT_SERIES_DEBUG=True) ---\")\n",
        "print_series_info_debug(mah_dist_series, \"mah_dist_series (full)\")\n",
        "print_series_info_debug(contrib_M_daily_df, \"contrib_M_daily_df (full)\")\n",
        "\n",
        "print(\"\\n--- Final Data Prep for Plotting & Attribution ---\")\n",
        "mah_dist_plot_final = mah_dist_series.dropna()\n",
        "vol_shock_plot_final = vol_shock_series.reindex(mah_dist_plot_final.index).fillna(0)\n",
        "corr_shock_plot_final = corr_shock_series.reindex(mah_dist_plot_final.index).fillna(0)\n",
        "valid_contrib_dates = mah_dist_plot_final.index\n",
        "contrib_M_daily_df = contrib_M_daily_df.loc[contrib_M_daily_df.index.isin(valid_contrib_dates)].dropna(how='all')\n",
        "contrib_VS_daily_df = contrib_VS_daily_df.loc[contrib_VS_daily_df.index.isin(valid_contrib_dates)].dropna(how='all')\n",
        "contrib_CS_daily_df = contrib_CS_daily_df.loc[contrib_CS_daily_df.index.isin(valid_contrib_dates)].dropna(how='all')\n",
        "print(f\"Length of final Mahalanobis Distance plot series: {len(mah_dist_plot_final)}\")\n",
        "if not mah_dist_plot_final.empty: print(f\"  Plot data from: {mah_dist_plot_final.index.min().strftime('%Y-%m-%d')} to {mah_dist_plot_final.index.max().strftime('%Y-%m-%d')}\")\n",
        "else: print(\"No data to plot for time series after all processing.\")\n",
        "\n",
        "# --- Plotting Time Series with Plotly (Single Figure, 3 Subplots) ---\n",
        "print(\"\\n--- Plotting Interactive Time Series with Plotly ---\")\n",
        "if not mah_dist_plot_final.empty:\n",
        "    fig_timeseries = make_subplots(rows=3, cols=1, shared_xaxes=True,\n",
        "                                   subplot_titles=('Mahalanobis Distance',\n",
        "                                                   'Weighted Correlation Shock Component',\n",
        "                                                   'Weighted Volatility Shock Component'))\n",
        "\n",
        "    # Subplot 1: Mahalanobis Distance\n",
        "    fig_timeseries.add_trace(go.Scatter(x=mah_dist_plot_final.index, y=mah_dist_plot_final, mode='lines', name='Mahalanobis Distance', line=dict(color='blue', width=1)), row=1, col=1)\n",
        "    m_dist_plot_threshold = mah_dist_plot_final.quantile(SHOCK_EVENT_QUANTILE)\n",
        "    fig_timeseries.add_hline(y=m_dist_plot_threshold, line_dash=\"dash\", line_color=\"red\", annotation_text=f\"{SHOCK_EVENT_QUANTILE*100:.0f}th Emp. Perc ({m_dist_plot_threshold:.2f})\", row=1, col=1)\n",
        "    fig_timeseries.update_yaxes(title_text=\"Mahalanobis Dist.\", row=1, col=1)\n",
        "\n",
        "    # Subplot 2: Correlation Shock Component\n",
        "    if not corr_shock_plot_final.empty:\n",
        "        fig_timeseries.add_trace(go.Scatter(x=corr_shock_plot_final.index, y=corr_shock_plot_final, mode='lines', name='Correlation Shock Comp.', line=dict(color='purple', width=1)), row=2, col=1)\n",
        "        threshold_cs_upper_val = corr_shock_plot_final.quantile(SHOCK_EVENT_QUANTILE)\n",
        "        threshold_cs_lower_val = corr_shock_plot_final.quantile(1-SHOCK_EVENT_QUANTILE)\n",
        "        fig_timeseries.add_hline(y=threshold_cs_upper_val, line_dash=\"dash\", line_color=\"red\", annotation_text=f\"{SHOCK_EVENT_QUANTILE*100:.0f}th Perc\", row=2, col=1)\n",
        "        fig_timeseries.add_hline(y=threshold_cs_lower_val, line_dash=\"dash\", line_color=\"orange\", annotation_text=f\"{(1-SHOCK_EVENT_QUANTILE)*100:.0f}th Perc\", row=2, col=1)\n",
        "    fig_timeseries.update_yaxes(title_text=\"Correlation Shock\", row=2, col=1)\n",
        "\n",
        "    # Subplot 3: Volatility Shock Component\n",
        "    if not vol_shock_plot_final.empty:\n",
        "        fig_timeseries.add_trace(go.Scatter(x=vol_shock_plot_final.index, y=vol_shock_plot_final, mode='lines', name='Volatility Shock Comp.', line=dict(color='green', width=1)), row=3, col=1)\n",
        "        threshold_vs_val = vol_shock_plot_final.quantile(SHOCK_EVENT_QUANTILE)\n",
        "        fig_timeseries.add_hline(y=threshold_vs_val, line_dash=\"dash\", line_color=\"red\", annotation_text=f\"{SHOCK_EVENT_QUANTILE*100:.0f}th Perc\", row=3, col=1)\n",
        "    fig_timeseries.update_yaxes(title_text=\"Volatility Shock\", row=3, col=1)\n",
        "\n",
        "    fig_timeseries.update_layout(height=900, title_text=f'Portfolio-Weighted Mahalanobis Distance & Decomposition (EWMA Half-life: {HALFLIFE} days)', legend_tracegroupgap=100)\n",
        "    fig_timeseries.update_xaxes(title_text=\"Date\", row=3, col=1)\n",
        "    print(\"Displaying combined interactive time series plot...\")\n",
        "    fig_timeseries.show()\n",
        "else:\n",
        "    print(\"Mahalanobis Distance time series plot not generated as data is empty.\")\n",
        "\n",
        "\n",
        "# --- Shock Day Attribution Analysis ---\n",
        "print(\"\\n--- Shock Day Attribution Analysis ---\")\n",
        "\n",
        "if mah_dist_plot_final.empty:\n",
        "    print(\"No Mahalanobis distance data available for shock day attribution.\")\n",
        "else:\n",
        "    shock_threshold_value = mah_dist_plot_final.quantile(SHOCK_EVENT_QUANTILE)\n",
        "    print(f\"Identifying shock days where Mahalanobis Distance >= {shock_threshold_value:.4f} (top {(1-SHOCK_EVENT_QUANTILE)*100:.0f}% of observed values).\")\n",
        "\n",
        "    shock_days_series = mah_dist_plot_final[mah_dist_plot_final >= shock_threshold_value].sort_index(ascending=False)\n",
        "\n",
        "    print(f\"Number of shock days found: {len(shock_days_series)}\")\n",
        "\n",
        "    if not shock_days_series.empty:\n",
        "        print(\"\\nShock Dates (Mahalanobis Distance >= Threshold), most recent first (all identified):\")\n",
        "        for date_val, value in shock_days_series.items():\n",
        "            print(f\"  - {date_val.strftime('%Y-%m-%d')}: Mahalanobis Distance = {value:.4f}\")\n",
        "\n",
        "        # --- Detailed Attribution Table for Top N Recent Shock Days ---\n",
        "        # This table remains as it provides a different level of detail (MD, VS, CS)\n",
        "        print(f\"\\nGenerating detailed attribution table for the {min(NUM_SHOCK_DAYS_FOR_TABLE_DETAIL, len(shock_days_series))} most recent shock day(s):\")\n",
        "        for k_shock_day_table, (shock_date_table, m_scalar_val_table) in enumerate(shock_days_series.head(NUM_SHOCK_DAYS_FOR_TABLE_DETAIL).items()):\n",
        "            print(f\"\\n--- Attribution Table for Shock Day: {shock_date_table.strftime('%Y-%m-%d')} (Mahalanobis Distance = {m_scalar_val_table:.4f}) ---\")\n",
        "            attribution_rows_for_table = []\n",
        "            periods_to_analyze = [('Last Day', 1), ('Avg Last Week (abs)', TRAILING_WEEK_DAYS), ('Avg Last Month (abs)', TRAILING_MONTH_DAYS)]\n",
        "            for period_label, num_days_in_period in periods_to_analyze:\n",
        "                C_M_k_period, C_VS_k_period, C_CS_k_period = [np.full(N_ASSETS, np.nan)] * 3\n",
        "                if shock_date_table not in contrib_M_daily_df.index: pass\n",
        "                elif num_days_in_period == 1:\n",
        "                    C_M_k_period = contrib_M_daily_df.loc[shock_date_table].values\n",
        "                    C_VS_k_period = contrib_VS_daily_df.loc[shock_date_table].values\n",
        "                    C_CS_k_period = contrib_CS_daily_df.loc[shock_date_table].values\n",
        "                else:\n",
        "                    try:\n",
        "                        if shock_date_table not in contrib_M_daily_df.index: raise KeyError\n",
        "                        idx_loc = contrib_M_daily_df.index.get_loc(shock_date_table)\n",
        "                        start_idx = max(0, idx_loc - num_days_in_period + 1)\n",
        "                        start_dt = contrib_M_daily_df.index[start_idx]; end_dt = shock_date_table\n",
        "                        if start_dt > end_dt: raise ValueError(\"Start date after end date\")\n",
        "                        m_slice = contrib_M_daily_df.loc[start_dt:end_dt]; vs_slice = contrib_VS_daily_df.loc[start_dt:end_dt]; cs_slice = contrib_CS_daily_df.loc[start_dt:end_dt]\n",
        "                        if not m_slice.empty: C_M_k_period = m_slice.abs().mean(skipna=True).values\n",
        "                        if not vs_slice.empty: C_VS_k_period = vs_slice.abs().mean(skipna=True).values\n",
        "                        if not cs_slice.empty: C_CS_k_period = cs_slice.abs().mean(skipna=True).values\n",
        "                    except (KeyError, ValueError): pass\n",
        "                attribution_rows_for_table.append({'Period': period_label, 'Contrib_M': C_M_k_period, 'Contrib_VS': C_VS_k_period, 'Contrib_CS': C_CS_k_period})\n",
        "            shock_day_table_list = []\n",
        "            for asset_idx, ticker_name in enumerate(tickers):\n",
        "                rd = {'Asset': ticker_name}\n",
        "                for r_detail in attribution_rows_for_table:\n",
        "                    p = r_detail['Period']\n",
        "                    def get_val(arr,ix): return arr[ix] if isinstance(arr,np.ndarray) and ix<len(arr) and not np.isnan(arr[ix]) else 'N/A'\n",
        "                    rd[f'C_MD ({p})']=get_val(r_detail['Contrib_M'],asset_idx); rd[f'C_VS ({p})']=get_val(r_detail['Contrib_VS'],asset_idx); rd[f'C_CS ({p})']=get_val(r_detail['Contrib_CS'],asset_idx)\n",
        "                shock_day_table_list.append(rd)\n",
        "            shock_day_contrib_df = pd.DataFrame(shock_day_table_list).set_index('Asset')\n",
        "            sort_col = 'C_MD (Last Day)'\n",
        "            if sort_col in shock_day_contrib_df.columns:\n",
        "                shock_day_contrib_df[sort_col] = pd.to_numeric(shock_day_contrib_df[sort_col], errors='coerce')\n",
        "                shock_day_contrib_df = shock_day_contrib_df.reindex(shock_day_contrib_df[sort_col].abs().sort_values(ascending=False).index).fillna('N/A')\n",
        "            print(shock_day_contrib_df.to_string(float_format=\"%.4f\"))\n",
        "\n",
        "        # --- NEW Consolidated Plotly Grouped Bar Chart for MD Contributions ---\n",
        "        print(f\"\\n--- Plotly Grouped Bar Chart of MD Contributions for Specific Dates ---\")\n",
        "\n",
        "        bar_chart_data_list = []\n",
        "        dates_for_bar_chart = {}\n",
        "\n",
        "        # 1. Latest Observation Day\n",
        "        if not contrib_M_daily_df.empty:\n",
        "            latest_obs_date = contrib_M_daily_df.index[-1]\n",
        "            dates_for_bar_chart[f\"Latest Obs. ({latest_obs_date.strftime('%Y-%m-%d')})\"] = contrib_M_daily_df.loc[latest_obs_date].values\n",
        "\n",
        "        # 2. Three most recent shock days (if available and different from latest_obs_date)\n",
        "        num_shock_days_added = 0\n",
        "        for shock_date, m_val in shock_days_series.items():\n",
        "            if num_shock_days_added >= NUM_RECENT_SHOCK_DAYS_FOR_BAR_CHART:\n",
        "                break\n",
        "            # Avoid duplicating latest_obs_date if it's also a shock day already plotted\n",
        "            date_label = f\"Shock Day ({shock_date.strftime('%Y-%m-%d')})\"\n",
        "            if date_label not in dates_for_bar_chart: # Ensure distinct columns for distinct scenarios\n",
        "                 if shock_date in contrib_M_daily_df.index:\n",
        "                    dates_for_bar_chart[date_label] = contrib_M_daily_df.loc[shock_date].values\n",
        "                    num_shock_days_added +=1\n",
        "                 elif latest_obs_date == shock_date and f\"Latest Obs. ({latest_obs_date.strftime('%Y-%m-%d')})\" in dates_for_bar_chart : #if latest obs was a shock day already counted\n",
        "                    pass #already added\n",
        "                 else: #shock date contributions not available\n",
        "                    dates_for_bar_chart[date_label] = np.full(N_ASSETS, np.nan)\n",
        "                    num_shock_days_added +=1\n",
        "\n",
        "\n",
        "        if not dates_for_bar_chart:\n",
        "            print(\"No data available for the consolidated contribution bar chart.\")\n",
        "        else:\n",
        "            bar_df_data = {'Asset': tickers}\n",
        "            for label, contrib_values in dates_for_bar_chart.items():\n",
        "                bar_df_data[label] = contrib_values\n",
        "\n",
        "            bar_df_plotly_consolidated = pd.DataFrame(bar_df_data).set_index('Asset')\n",
        "            # Sort assets by max absolute contribution across the displayed scenarios for better visualization\n",
        "            if not bar_df_plotly_consolidated.empty:\n",
        "                 bar_df_plotly_consolidated['max_abs_contrib'] = bar_df_plotly_consolidated.abs().max(axis=1)\n",
        "                 bar_df_plotly_consolidated = bar_df_plotly_consolidated.sort_values(by='max_abs_contrib', ascending=False).drop(columns=['max_abs_contrib'])\n",
        "\n",
        "            fig_contrib_bar_consolidated = go.Figure()\n",
        "            for period_col_name_bar in bar_df_plotly_consolidated.columns:\n",
        "                fig_contrib_bar_consolidated.add_trace(go.Bar(\n",
        "                    name=period_col_name_bar,\n",
        "                    x=bar_df_plotly_consolidated.index,\n",
        "                    y=bar_df_plotly_consolidated[period_col_name_bar]\n",
        "                ))\n",
        "\n",
        "            fig_contrib_bar_consolidated.update_layout(\n",
        "                barmode='group',\n",
        "                title_text=f'Asset Contributions to Mahalanobis Distance for Key Dates',\n",
        "                xaxis_title=\"Asset\",\n",
        "                yaxis_title=\"Contribution to Mahalanobis Distance ($C_{M,k}$)\",\n",
        "                legend_title=\"Date/Scenario\"\n",
        "            )\n",
        "            fig_contrib_bar_consolidated.show()\n",
        "\n",
        "    else:\n",
        "        print(\"No shock days met threshold for detailed attribution or plotting.\")\n",
        "\n",
        "print(\"\\n--- Analysis Complete ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Claude Refactoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from functools import reduce\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'tickers': ['XLE', 'XLF', 'XLU', 'XLI', 'XLK', 'XLV', 'XLY', 'XLP', 'XLB'],\n",
        "    'start_date': '2004-01-01',\n",
        "    'end_date': pd.Timestamp.today().strftime('%Y-%m-%d'),\n",
        "    'halflife': 126,\n",
        "    'min_periods': 126,\n",
        "    'shock_quantile': 0.99,\n",
        "    'num_shock_days_table': 3,\n",
        "    'num_shock_days_chart': 3,\n",
        "    'trailing_periods': {'week': 5, 'month': 21}\n",
        "}\n",
        "\n",
        "# Basic utility functions\n",
        "def is_valid_covariance(cov_matrix: np.ndarray, max_condition: float = 1e12) -> bool:\n",
        "    \"\"\"Check if covariance matrix is valid for inversion.\"\"\"\n",
        "    return (\n",
        "        not np.isnan(cov_matrix).any() and\n",
        "        not np.isinf(cov_matrix).any() and\n",
        "        np.all(np.diag(cov_matrix) > 1e-12) and\n",
        "        np.linalg.cond(cov_matrix) <= max_condition\n",
        "    )\n",
        "\n",
        "# Data acquisition and preprocessing\n",
        "def download_data(tickers: list, start: str, end: str) -> pd.DataFrame:\n",
        "    \"\"\"Download and clean price data.\"\"\"\n",
        "    print(f\"Downloading data for {len(tickers)} tickers from {start} to {end}\")\n",
        "    \n",
        "    raw_data = yf.download(tickers, start=start, end=end, progress=False)\n",
        "    \n",
        "    if raw_data.empty:\n",
        "        raise ValueError(\"No data downloaded\")\n",
        "    \n",
        "    # Extract close prices\n",
        "    if isinstance(raw_data.columns, pd.MultiIndex):\n",
        "        data = raw_data.xs('Close', level=0, axis=1)\n",
        "    elif len(tickers) == 1:\n",
        "        data = raw_data[['Close']].rename(columns={'Close': tickers[0]})\n",
        "    else:\n",
        "        data = raw_data[tickers]\n",
        "    \n",
        "    # Clean and validate\n",
        "    data = data.dropna()\n",
        "    if len(data) < CONFIG['min_periods'] + 5:\n",
        "        raise ValueError(f\"Insufficient data: {len(data)} rows\")\n",
        "    \n",
        "    print(f\"Data shape: {data.shape}, range: {data.index.min():%Y-%m-%d} to {data.index.max():%Y-%m-%d}\")\n",
        "    return data\n",
        "\n",
        "def calculate_returns(prices: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Calculate log returns.\"\"\"\n",
        "    returns = np.log(prices / prices.shift(1)).dropna()\n",
        "    print(f\"Returns shape: {returns.shape}\")\n",
        "    return returns\n",
        "\n",
        "def calculate_ewma_covariance(returns: pd.DataFrame, halflife: int, min_periods: int) -> pd.DataFrame:\n",
        "    \"\"\"Calculate EWMA covariance matrix series.\"\"\"\n",
        "    return returns.ewm(halflife=halflife, min_periods=min_periods).cov()\n",
        "\n",
        "# Core risk metric calculations\n",
        "def calculate_metrics_for_date(\n",
        "    returns_t: np.ndarray, \n",
        "    cov_matrix: np.ndarray, \n",
        "    weights: np.ndarray\n",
        ") -> tuple[float, float, float, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Calculate all metrics for a single date.\"\"\"\n",
        "    W_diag = np.diag(weights)\n",
        "    cov_inv = np.linalg.inv(cov_matrix)\n",
        "    \n",
        "    # Mahalanobis distance\n",
        "    mahal_dist = (returns_t.T @ W_diag @ cov_inv @ W_diag @ returns_t).item()\n",
        "    \n",
        "    # Volatility shock (using diagonal covariance)\n",
        "    vol_inv = np.diag(1.0 / np.diag(cov_matrix))\n",
        "    vol_shock = (returns_t.T @ W_diag @ vol_inv @ W_diag @ returns_t).item()\n",
        "    \n",
        "    # Correlation shock (residual)\n",
        "    corr_shock = mahal_dist - vol_shock\n",
        "    \n",
        "    # Contributions\n",
        "    A_matrix = W_diag @ cov_inv @ W_diag\n",
        "    v_vector = A_matrix @ returns_t\n",
        "    contrib_mahal = returns_t.flatten() * v_vector.flatten()\n",
        "    \n",
        "    std_devs = np.sqrt(np.diag(cov_matrix))\n",
        "    contrib_vol = (weights**2) * ((returns_t.flatten() / std_devs)**2)\n",
        "    contrib_corr = contrib_mahal - contrib_vol\n",
        "    \n",
        "    return mahal_dist, vol_shock, corr_shock, contrib_mahal, contrib_vol, contrib_corr\n",
        "\n",
        "def compute_risk_metrics(returns: pd.DataFrame, cov_series: pd.DataFrame) -> dict[str, pd.Series]:\n",
        "    \"\"\"Compute all risk metrics efficiently.\"\"\"\n",
        "    tickers = returns.columns.tolist()\n",
        "    n_assets = len(tickers)\n",
        "    weights = np.full(n_assets, 1/n_assets)\n",
        "    \n",
        "    # Initialize result containers\n",
        "    results = {\n",
        "        'mahal_dist': pd.Series(index=returns.index, dtype=float),\n",
        "        'vol_shock': pd.Series(index=returns.index, dtype=float),\n",
        "        'corr_shock': pd.Series(index=returns.index, dtype=float),\n",
        "        'contrib_mahal': pd.DataFrame(index=returns.index, columns=tickers, dtype=float),\n",
        "        'contrib_vol': pd.DataFrame(index=returns.index, columns=tickers, dtype=float),\n",
        "        'contrib_corr': pd.DataFrame(index=returns.index, columns=tickers, dtype=float)\n",
        "    }\n",
        "    \n",
        "    valid_cov_dates = cov_series.index.get_level_values(0).unique()\n",
        "    processed = 0\n",
        "    \n",
        "    for i in range(1, len(returns)):\n",
        "        t_date = returns.index[i]\n",
        "        t_minus_1 = returns.index[i-1]\n",
        "        \n",
        "        if t_minus_1 not in valid_cov_dates:\n",
        "            continue\n",
        "            \n",
        "        try:\n",
        "            cov_matrix = cov_series.loc[t_minus_1].reindex(\n",
        "                index=tickers, columns=tickers\n",
        "            ).values\n",
        "            \n",
        "            if not is_valid_covariance(cov_matrix):\n",
        "                continue\n",
        "                \n",
        "            returns_t = returns.loc[t_date].values.reshape(-1, 1)\n",
        "            \n",
        "            metrics = calculate_metrics_for_date(returns_t, cov_matrix, weights)\n",
        "            mahal_dist, vol_shock, corr_shock, contrib_m, contrib_v, contrib_c = metrics\n",
        "            \n",
        "            results['mahal_dist'].loc[t_date] = mahal_dist\n",
        "            results['vol_shock'].loc[t_date] = vol_shock\n",
        "            results['corr_shock'].loc[t_date] = corr_shock\n",
        "            results['contrib_mahal'].loc[t_date] = contrib_m\n",
        "            results['contrib_vol'].loc[t_date] = contrib_v\n",
        "            results['contrib_corr'].loc[t_date] = contrib_c\n",
        "            \n",
        "            processed += 1\n",
        "            \n",
        "        except Exception:\n",
        "            continue\n",
        "    \n",
        "    print(f\"Successfully processed {processed} days\")\n",
        "    return {k: v.dropna() if isinstance(v, pd.Series) else v.dropna(how='all') \n",
        "            for k, v in results.items()}\n",
        "\n",
        "# Business logic and analysis\n",
        "def identify_shock_days(mahal_series: pd.Series, quantile: float) -> pd.Series:\n",
        "    \"\"\"Identify shock days above threshold.\"\"\"\n",
        "    threshold = mahal_series.quantile(quantile)\n",
        "    shock_days = mahal_series[mahal_series >= threshold].sort_index(ascending=False)\n",
        "    print(f\"Found {len(shock_days)} shock days (threshold: {threshold:.4f})\")\n",
        "    return shock_days\n",
        "\n",
        "def create_attribution_table(\n",
        "    shock_date: pd.Timestamp,\n",
        "    contrib_data: dict[str, pd.DataFrame],\n",
        "    tickers: list,\n",
        "    periods: dict[str, int]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Create attribution table for a shock day.\"\"\"\n",
        "    def get_period_contrib(df: pd.DataFrame, date: pd.Timestamp, days: int) -> np.ndarray:\n",
        "        if date not in df.index:\n",
        "            return np.full(len(tickers), np.nan)\n",
        "        \n",
        "        if days == 1:\n",
        "            return df.loc[date].values\n",
        "        \n",
        "        try:\n",
        "            idx = df.index.get_loc(date)\n",
        "            start_idx = max(0, idx - days + 1)\n",
        "            slice_data = df.iloc[start_idx:idx+1]\n",
        "            return slice_data.abs().mean().values\n",
        "        except:\n",
        "            return np.full(len(tickers), np.nan)\n",
        "    \n",
        "    # Calculate contributions for different periods\n",
        "    period_data = {}\n",
        "    for period_name, days in [('Last Day', 1)] + [(f'Avg {k.title()}', v) for k, v in periods.items()]:\n",
        "        for metric in ['mahal', 'vol', 'corr']:\n",
        "            contrib_key = f'contrib_{metric}'\n",
        "            if contrib_key in contrib_data:\n",
        "                period_data[f'{metric}_{period_name}'] = get_period_contrib(\n",
        "                    contrib_data[contrib_key], shock_date, days\n",
        "                )\n",
        "    \n",
        "    # Create DataFrame\n",
        "    table_data = []\n",
        "    for i, ticker in enumerate(tickers):\n",
        "        row = {'Asset': ticker}\n",
        "        for col_name, values in period_data.items():\n",
        "            row[col_name] = values[i] if not np.isnan(values[i]) else 'N/A'\n",
        "        table_data.append(row)\n",
        "    \n",
        "    df = pd.DataFrame(table_data).set_index('Asset')\n",
        "    \n",
        "    # Sort by absolute contribution\n",
        "    if 'mahal_Last Day' in df.columns:\n",
        "        sort_col = pd.to_numeric(df['mahal_Last Day'], errors='coerce')\n",
        "        df = df.reindex(sort_col.abs().sort_values(ascending=False).index)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Visualization and reporting\n",
        "def create_time_series_plot(metrics: dict[str, pd.Series], shock_quantile: float) -> go.Figure:\n",
        "    \"\"\"Create interactive time series plot.\"\"\"\n",
        "    fig = make_subplots(\n",
        "        rows=3, cols=1, shared_xaxes=True,\n",
        "        subplot_titles=['Mahalanobis Distance', 'Correlation Shock', 'Volatility Shock']\n",
        "    )\n",
        "    \n",
        "    # Mahalanobis distance\n",
        "    mahal = metrics['mahal_dist']\n",
        "    threshold = mahal.quantile(shock_quantile)\n",
        "    \n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=mahal.index, y=mahal, mode='lines', \n",
        "        name='Mahalanobis Distance', line=dict(color='blue')\n",
        "    ), row=1, col=1)\n",
        "    \n",
        "    fig.add_hline(\n",
        "        y=threshold, line_dash=\"dash\", line_color=\"red\",\n",
        "        annotation_text=f\"{shock_quantile*100:.0f}th percentile ({threshold:.2f})\",\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # Correlation shock\n",
        "    corr_shock = metrics['corr_shock']\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=corr_shock.index, y=corr_shock, mode='lines',\n",
        "        name='Correlation Shock', line=dict(color='purple')\n",
        "    ), row=2, col=1)\n",
        "    \n",
        "    # Volatility shock\n",
        "    vol_shock = metrics['vol_shock']\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=vol_shock.index, y=vol_shock, mode='lines',\n",
        "        name='Volatility Shock', line=dict(color='green')\n",
        "    ), row=3, col=1)\n",
        "    \n",
        "    fig.update_layout(\n",
        "        height=900, \n",
        "        title=f'Portfolio Risk Metrics (EWMA Half-life: {CONFIG[\"halflife\"]} days)',\n",
        "        showlegend=True\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "def create_contribution_bar_chart(\n",
        "    contrib_data: pd.DataFrame,\n",
        "    shock_days: pd.Series,\n",
        "    num_shock_days: int\n",
        ") -> go.Figure:\n",
        "    \"\"\"Create grouped bar chart for contributions.\"\"\"\n",
        "    chart_data = {}\n",
        "    \n",
        "    # Latest observation\n",
        "    if not contrib_data.empty:\n",
        "        latest_date = contrib_data.index[-1]\n",
        "        chart_data[f\"Latest ({latest_date:%Y-%m-%d})\"] = contrib_data.loc[latest_date]\n",
        "    \n",
        "    # Recent shock days\n",
        "    for i, (shock_date, _) in enumerate(shock_days.head(num_shock_days).items()):\n",
        "        if shock_date in contrib_data.index:\n",
        "            chart_data[f\"Shock ({shock_date:%Y-%m-%d})\"] = contrib_data.loc[shock_date]\n",
        "    \n",
        "    if not chart_data:\n",
        "        return go.Figure()\n",
        "    \n",
        "    # Create bar chart\n",
        "    fig = go.Figure()\n",
        "    \n",
        "    for label, values in chart_data.items():\n",
        "        fig.add_trace(go.Bar(name=label, x=values.index, y=values.values))\n",
        "    \n",
        "    fig.update_layout(\n",
        "        barmode='group',\n",
        "        title='Asset Contributions to Mahalanobis Distance',\n",
        "        xaxis_title='Asset',\n",
        "        yaxis_title='Contribution',\n",
        "        legend_title='Date'\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Main orchestration\n",
        "def main():\n",
        "    \"\"\"Main analysis pipeline.\"\"\"\n",
        "    try:\n",
        "        # Data acquisition and processing\n",
        "        prices = download_data(CONFIG['tickers'], CONFIG['start_date'], CONFIG['end_date'])\n",
        "        returns = calculate_returns(prices)\n",
        "        cov_series = calculate_ewma_covariance(returns, CONFIG['halflife'], CONFIG['min_periods'])\n",
        "        \n",
        "        # Risk metrics calculation\n",
        "        print(\"\\nCalculating risk metrics...\")\n",
        "        metrics = compute_risk_metrics(returns, cov_series)\n",
        "        \n",
        "        # Time series visualization\n",
        "        print(\"\\nCreating time series plot...\")\n",
        "        ts_fig = create_time_series_plot(metrics, CONFIG['shock_quantile'])\n",
        "        ts_fig.show()\n",
        "        \n",
        "        # Shock day analysis\n",
        "        print(\"\\nAnalyzing shock days...\")\n",
        "        shock_days = identify_shock_days(metrics['mahal_dist'], CONFIG['shock_quantile'])\n",
        "        \n",
        "        if not shock_days.empty:\n",
        "            # Attribution tables\n",
        "            for i, (shock_date, mahal_val) in enumerate(shock_days.head(CONFIG['num_shock_days_table']).items()):\n",
        "                print(f\"\\nShock Day {i+1}: {shock_date:%Y-%m-%d} (Mahalanobis: {mahal_val:.4f})\")\n",
        "                \n",
        "                table = create_attribution_table(\n",
        "                    shock_date, metrics, CONFIG['tickers'], CONFIG['trailing_periods']\n",
        "                )\n",
        "                print(table.to_string(float_format=\"%.4f\"))\n",
        "            \n",
        "            # Contribution bar chart\n",
        "            print(\"\\nCreating contribution bar chart...\")\n",
        "            bar_fig = create_contribution_bar_chart(\n",
        "                metrics['contrib_mahal'], shock_days, CONFIG['num_shock_days_chart']\n",
        "            )\n",
        "            bar_fig.show()\n",
        "        \n",
        "        print(\"\\nAnalysis complete!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in analysis: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from risk_data import get_factor_data\n",
        "import xarray as xr\n",
        "factor_data = get_factor_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = factor_data.sel(factor_name=CONFIG['tickers'], factor_name_1=CONFIG['tickers'], date=slice(CONFIG['start_date'], CONFIG['end_date']), vol_type=CONFIG['halflife'], corr_type=CONFIG['halflife'])\n",
        "ret = ds['ret'].to_pandas()/10000\n",
        "\n",
        "def pd_diag(ser: pd.Series) -> pd.DataFrame:\n",
        "    return pd.DataFrame(np.diag(ser), index=ser.index, columns=ser.index)\n",
        "\n",
        "def get_cov(vol: pd.Series, corr: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Construct covariance matrix from volatilities and correlations.\"\"\"\n",
        "    D = pd_diag(vol)\n",
        "    return D @ corr @ D\n",
        "\n",
        "ds_t = ds.sel(date=ds.date.max())\n",
        "cov = get_cov(vol=ds_t['vol'].to_series(), \n",
        "              corr=ds_t['corr'].to_pandas())\n",
        "cov\n",
        "# ds.sel(date=ds.date.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "returns2 = ds.ret.to_pandas()\n",
        "cov_series2 = calculate_ewma_covariance(returns2, CONFIG['halflife'], CONFIG['min_periods'])\n",
        "cov_series2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data acquisition and processing\n",
        "prices = download_data(CONFIG['tickers'], CONFIG['start_date'], CONFIG['end_date'])\n",
        "returns = calculate_returns(prices)\n",
        "cov_series = calculate_ewma_covariance(returns, CONFIG['halflife'], CONFIG['min_periods'])\n",
        "cov_series\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    \n",
        "    # Risk metrics calculation\n",
        "    print(\"\\nCalculating risk metrics...\")\n",
        "    metrics = compute_risk_metrics(returns, cov_series)\n",
        "        \n",
        "    # Time series visualization\n",
        "    print(\"\\nCreating time series plot...\")\n",
        "    ts_fig = create_time_series_plot(metrics, CONFIG['shock_quantile'])\n",
        "    ts_fig.show()\n",
        "        \n",
        "    # Shock day analysis\n",
        "    print(\"\\nAnalyzing shock days...\")\n",
        "    shock_days = identify_shock_days(metrics['mahal_dist'], CONFIG['shock_quantile'])\n",
        "        \n",
        "        if not shock_days.empty:\n",
        "            # Attribution tables\n",
        "            for i, (shock_date, mahal_val) in enumerate(shock_days.head(CONFIG['num_shock_days_table']).items()):\n",
        "                print(f\"\\nShock Day {i+1}: {shock_date:%Y-%m-%d} (Mahalanobis: {mahal_val:.4f})\")\n",
        "                \n",
        "                table = create_attribution_table(\n",
        "                    shock_date, metrics, CONFIG['tickers'], CONFIG['trailing_periods']\n",
        "                )\n",
        "                print(table.to_string(float_format=\"%.4f\"))\n",
        "            \n",
        "    #         # Contribution bar chart\n",
        "    #         print(\"\\nCreating contribution bar chart...\")\n",
        "    #         bar_fig = create_contribution_bar_chart(\n",
        "    #             metrics['contrib_mahal'], shock_days, CONFIG['num_shock_days_chart']\n",
        "    #         )\n",
        "    #         bar_fig.show()\n",
        "        \n",
        "    #     print(\"\\nAnalysis complete!\")\n",
        "        \n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error in analysis: {e}\")\n",
        "    #     raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "xr.Dataset(metrics)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-bklm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
